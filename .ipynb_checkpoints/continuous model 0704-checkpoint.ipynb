{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8c0e63-aedf-4411-b589-750c8d241236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2023-07-04\n",
      "2023-07-04 17:39:42.297973\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SubsetRandomSampler,Sampler\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import colorama\n",
    "from colorama import Fore,Back,Style\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Today's date:\",date.today())\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a060fe-79d5-4f79-96ba-ee3ea139bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = '3diff_easy_negative_3rep'\n",
    "# # OUT_DIR = '2023-05-17_09-46-34_tag11_mark0.946Epoch20'\n",
    "BATCH_SIZE = 1\n",
    "LR = 0.005\n",
    "EPOCH = 500\n",
    "CUTOFF = 1800\n",
    "TAG = 1\n",
    "SEED = 44\n",
    "T = 0.005\n",
    "LAMBDA = 0\n",
    "w = 100\n",
    "LIMIT = 5\n",
    "NPY_DIR ='/project/DPDS/Wang_lab/shared/BCR_antigen/data/rfscript/Antigen_embed/CLEANED'\n",
    "VERBOSE = False\n",
    "GROUP_SIZE = 50\n",
    "Max_num = 2\n",
    "Small_sample = None\n",
    "EX_SUBSAMPLE = 1\n",
    "#IN_SUBSAMPLE = 0.033 # not used. \n",
    "# specified different IN_SUBSAMPLE for bin and cont data, \n",
    "# so that we have mostly cont data for the cont model\n",
    "CHANNEL_ANTIGEN = 600\n",
    "MODEL = None\n",
    "CLIP = None\n",
    "# #INPUT = '3diff_easy_negative_5rep'\n",
    "INPUT_E = 'exVal_r4'\n",
    "EACH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b8603a-f0bb-431c-b3d2-20d21cda1d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n",
      "[GCC 10.3.0]\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "os.chdir('/project/DPDS/Wang_lab/shared/BCR_antigen/code/DeLAnO/')\n",
    "#mp.set_sharing_strategy('file_system')\n",
    "#mp.set_start_method('spawn')\n",
    "from wrapV.Vwrap import embedV\n",
    "from wrapCDR3.CDR3wrap import embedCDR3\n",
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abe08db-3153-4213-a4b1-6d90d9205df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Inputing original datasets\n",
    "input_ori = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/3diff_easy_negative_3rep.csv',index_col = 0)\n",
    "total_N_row = input_ori.shape[0]\n",
    "\n",
    "#exVal_ori = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/results_hard_negative/results_hard_negative.csv')\n",
    "exVal_ori = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/'+INPUT_E+'.csv',index_col=0)\n",
    "total_N_ex = exVal_ori.shape[0]\n",
    "\n",
    "cont_val_ori = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/train_validate/BCR_optimization/better_vs_worse_BCRs_forBing.txt',sep = '\\t')\n",
    "# cont_val.head()\n",
    "cont_train_10x = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/train_validate/continuous_training/cont_training_10x_margin2.csv')\n",
    "cont_train_libra = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/train_validate/continuous_training/cont_training_libra_margin0.5.csv')\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "## Replace Hie/EbolaGP's sequence to the short one\n",
    "cont_val_ori.iloc[13,7]='MGVTGILQLPRDRFKRTSFFLWVIILFQRTFSIPLGVIHNSTLQVSDVDKLVCRDKLSSTNQLRSVGLNLEGNGVATDVPSATKRWGFRSGVPPKVVNYEAGEWAENCYNLEIKKPDGSECLPAAPDGIRGFPRCRYVHKVSGTGPCAGDFAFHKEGAFFLYDRLASTVIYRGTTFAEGVVAFLILPQAKKDFFSSHPLREPVNATEDPSSGYYSTTIRYQATGFGTNETEYLFEVDNLTYVQLESRFTPQFLLQLNETIYTSGKRSNTTGKLIWKVNPEIDTTIGEWAFWETKKNLTRKIRSEELSFAGLITGGRRTRREAIVNAQPKCNPNLHYWTTQDEGAAIGLAWIPYFGPAAEGIYIEGLMHNQDGLICGLRQLANETTQALQLFLRATTELRTFSILNRKAIDFLLQRWGGTCHILGPDCCIEPHDWTKNITDKIDQIIHDFVDKTLPDQGDNDNWWTGWRQWIPAGIGVTGVIIAVIALFCICKFVF'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a1a83c-cbe1-4225-8f67-e0a71e2118a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2547, 13)\n",
      "(404, 13)\n",
      "(11436, 13)\n",
      "(4741, 13)\n"
     ]
    }
   ],
   "source": [
    "print(cont_train_libra.shape)\n",
    "cont_train_libra=cont_train_libra[cont_train_libra[\"distance\"]<6]\n",
    "print(cont_train_libra.shape)\n",
    "\n",
    "print(cont_train_10x.shape)\n",
    "cont_train_10x=cont_train_10x[cont_train_10x[\"distance\"]<6]\n",
    "print(cont_train_10x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b4eafb-ef8f-4a9f-9811-7e58d335ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/project/DPDS/Wang_lab/shared/BCR_antigen/data/toInput/easy_V_dict.pkl','rb') as f:\n",
    "    Vh_dict = pickle.load(f)\n",
    "with open('/project/DPDS/Wang_lab/shared/BCR_antigen/data/toInput/easy_CDR3_dict.pkl','rb') as f:\n",
    "    CDR3h_dict = pickle.load(f)\n",
    "with open('/project/DPDS/Wang_lab/shared/BCR_antigen/data/toInput/hard_V_dict.pkl', 'rb') as f:\n",
    "    Vh_hard_dict = pickle.load(f)\n",
    "with open('/project/DPDS/Wang_lab/shared/BCR_antigen/data/toInput/hard_CDR3_dict.pkl', 'rb') as f:\n",
    "    CDR3h_hard_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7395bb7b-5d87-4be8-b93e-7b98cab9e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "\n",
    "# def functions\n",
    "\n",
    "def get_now():\n",
    "    now = str(datetime.now())\n",
    "    tosec = now.split('.')[0]\n",
    "    out_now = tosec.split(' ')[0]+'_'+tosec.split(' ')[1].replace(':',\"-\")\n",
    "    return out_now\n",
    "\n",
    "def filter_big_antigens(dataset,cutoff):\n",
    "    dataset['aalens']=list(map(len,dataset['Antigen'].tolist()))\n",
    "    data_filtered = dataset.loc[dataset['aalens']< cutoff]\n",
    "    print('After removing antigens larger than '+str(cutoff)+', '+str(100*data_filtered.shape[0]/dataset.shape[0])+'% antigens remained.')\n",
    "    return data_filtered\n",
    "\n",
    "def check_bad_bcr(seq):\n",
    "    allowed_letters = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    uppercase_string = seq.upper()\n",
    "    return not any(char not in allowed_letters for char in uppercase_string)\n",
    "\n",
    "def filter_bad_bcr(df,binary = True):\n",
    "    if binary:\n",
    "        mask = df[['BetterBCR_Vh', 'BetterBCR_CDR3h', 'WorseBCR_Vh', 'WorseBCR_CDR3h']].applymap(check_bad_bcr)\n",
    "    else:\n",
    "        mask = df[['BetterBCR_Vh', 'BetterBCR_CDR3h']].applymap(check_bad_bcr)\n",
    "    filtered_df = df[mask.all(axis=1)]\n",
    "    return filtered_df\n",
    "\n",
    "def preprocess(df,cutoff,binary = True):\n",
    "    if binary:\n",
    "        for col in df[['BetterBCR_Vh','BetterBCR_CDR3h','WorseBCR_Vh','WorseBCR_CDR3h','Antigen']].columns:\n",
    "            df[col] = df[col].apply(lambda x: x.replace(' ', ''))\n",
    "    else:\n",
    "        for col in df[['BetterBCR_Vh','BetterBCR_CDR3h','Antigen']].columns:\n",
    "            df[col] = df[col].apply(lambda x: x.replace(' ', ''))\n",
    "    df =  filter_bad_bcr(df,binary = binary)\n",
    "    df = filter_big_antigens(df,cutoff)\n",
    "    df = df.assign(antigen_index = df['Project'] + '/' + df['id'].astype(str))\n",
    "    df = df.sort_values('antigen_index',)\n",
    "    df = df.assign(record_id = ['record_' + str(s) for s in range(df.shape[0])])\n",
    "    return df\n",
    "\n",
    "# def has_space(string):\n",
    "#     return \" \" in string\n",
    "def check_bad_letters(df,binary = True):\n",
    "    if binary:\n",
    "        df = df[['BetterBCR_Vh','BetterBCR_CDR3h','WorseBCR_Vh','WorseBCR_CDR3h','Antigen']]\n",
    "    else:\n",
    "        df = df[['BetterBCR_Vh','BetterBCR_CDR3h','Antigen']]\n",
    "    for col in df.columns:\n",
    "        if not all(df[col].apply(check_bad_bcr)):\n",
    "            print(Fore.RED +str(col)+' contains uncommon aa or symbols!')\n",
    "        else:\n",
    "            print(Fore.GREEN +str(col)+' PASS!')\n",
    "    print(Style.RESET_ALL)\n",
    "\n",
    "def build_BCR_dict(dataset,colname,precise = False):\n",
    "    cols = dataset.filter(like = colname)\n",
    "    uniq_keys = pd.unique(cols.values.ravel()).tolist()\n",
    "    if colname == 'CDR3h':\n",
    "        uniq_embedding,_,input_keys = embedCDR3(uniq_keys,precise = precise)\n",
    "    elif colname == 'Vh':\n",
    "        uniq_embedding,_,input_keys = embedV(uniq_keys,precise = precise)\n",
    "    i = 0\n",
    "    mydict = {}\n",
    "    for key in input_keys:\n",
    "        mydict[key] = uniq_embedding[i]\n",
    "        i += 1\n",
    "    return(mydict)\n",
    "\n",
    "def predict_size(len,datatype = 'float32'):\n",
    "    # Define the shape of the tensor\n",
    "    shape = [1, len, len, 318]\n",
    "    element_size = np.dtype(datatype).itemsize\n",
    "    num_elements = math.prod(shape)\n",
    "    tensor_size = num_elements * element_size\n",
    "#    size_gb = tensor_size/(1024**3)\n",
    "#    print(f\"The tensor takes up {size_gb:.2f} GB of memory.\")\n",
    "#    size_gb = tensor_size/(1024*1024*1024)\n",
    "#    print(f\"Tensor size: {size_gb:.2f} GB\")\n",
    "    return tensor_size\n",
    "\n",
    "# for example, subsample is around 10000, each antigen 50 +-10;\n",
    "# df = input_ori\n",
    "def small_data(df,subsample,each = 50,around = 10):\n",
    "    subsample = min(subsample,df.shape[0])\n",
    "    df['antigen_index']= df['Project']+'/'+df['id']\n",
    "    antigen_ls = df['antigen_index'].unique().tolist()\n",
    "    num_to_pick = subsample//each\n",
    "    antigen_to_pick = random.sample(antigen_ls,min(num_to_pick,len(antigen_ls)))\n",
    "    #selected_types = random.sample(df['type'].unique().tolist(), min(num_types_to_select, len(df['type'].unique())))\n",
    "    # Initialize an empty dataframe to store the selected rows\n",
    "    selected_rows = pd.DataFrame()\n",
    "\n",
    "    # Loop through the selected types\n",
    "    for t in antigen_to_pick:\n",
    "        antigen_rows = df[df['antigen_index'] == t]\n",
    "\n",
    "        # Select 40-60 rows for each type, or all rows if there are not enough\n",
    "        num_rows_to_select = min(len(antigen_rows), random.randint(each-around, each+around))\n",
    "        selected_antigen_rows = antigen_rows.sample(num_rows_to_select)\n",
    "\n",
    "        # Append the selected rows to the final dataframe\n",
    "        selected_rows = pd.concat((selected_rows,selected_antigen_rows),axis=0)\n",
    "\n",
    "    # Reset the index of the final dataframe\n",
    "    selected_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print the final dataframe\n",
    "    return selected_rows\n",
    "\n",
    "def small_data_in(df,each = 3, around = 1):\n",
    "    df['antigen_index']= df['Project']+'/'+df['id']\n",
    "    df['good_pair']= df['antigen_index']+'_'+df['BetterBCR_Vh']+'_'+df['BetterBCR_CDR3h']\n",
    "    pair_list = df['good_pair'].unique().tolist()\n",
    "    selected_rows = pd.DataFrame()\n",
    "\n",
    "    # Loop through the selected types\n",
    "    for t in pair_list:\n",
    "        pair_rows = df[df['good_pair'] == t]\n",
    "\n",
    "        # Select 40-60 rows for each type, or all rows if there are not enough\n",
    "        num_rows_to_select = min(len(pair_rows), random.randint(each-around, each+around))\n",
    "        selected_pair_rows = pair_rows.sample(num_rows_to_select)\n",
    "\n",
    "        # Append the selected rows to the final dataframe\n",
    "        selected_rows = pd.concat((selected_rows,selected_pair_rows),axis=0)\n",
    "\n",
    "    # Reset the index of the final dataframe\n",
    "    selected_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print the final dataframe\n",
    "    return selected_rows\n",
    "\n",
    "\n",
    "# In[90]:\n",
    "\n",
    "\n",
    "def gen_train_val_loader(dataloader,batchsize=BATCH_SIZE,validate=True):\n",
    "    train_dataset = dataloader.get_train_dataset()\n",
    "    train_loader = DataLoader(train_dataset, batchsize, sampler=GroupShuffleSampler(train_dataset, train_dataset.group_ids))\n",
    "    if validate:\n",
    "        val_dataset = dataloader.get_val_dataset()\n",
    "        val_loader = DataLoader(val_dataset, batchsize, sampler=GroupShuffleSampler(val_dataset, val_dataset.group_ids))\n",
    "        return train_loader,val_loader\n",
    "    else:\n",
    "        return train_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6bcedb1-59c9-4539-946f-c253097af56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, dataframe, antigen_fpath_dict, cdr3_dict, v_dict, subsample_ratio = 1.0):\n",
    "        self.dataframe = self.subsample_data(dataframe, subsample_ratio)\n",
    "        self.group_ids = self.generate_group_index(group_size=GROUP_SIZE)  # Adjust the group_size as needed\n",
    "        self.your_data_list = self.get_my_data_list()\n",
    "        self.antigen_fpath_dict = antigen_fpath_dict\n",
    "#        self.antigen_fpath_dict = self.__read_files()\n",
    "#        self.your_data_list = your_data_list\n",
    "        self.antigen_dict = {}\n",
    "        self.cdr3_dict = cdr3_dict\n",
    "        self.v_dict = v_dict\n",
    "        self.antigen_in = {}\n",
    "        self.lens_dict = {}\n",
    "\n",
    "    def subsample_data(self, dataframe, subsample_ratio):\n",
    "        if subsample_ratio < 1.0:\n",
    "            return dataframe.sample(frac=subsample_ratio)\n",
    "        else:\n",
    "            return dataframe\n",
    "    def __getitem__(self, idx):\n",
    "        your_dict = self.your_data_list[idx]\n",
    "        antigen_key = your_dict['antigen_index']\n",
    "        aalens_key = your_dict['aalens']\n",
    "        self.lens_dict[antigen_key]=aalens_key\n",
    "        betterCDR_key = your_dict['BetterBCR_CDR3h']\n",
    "        betterV_key = your_dict['BetterBCR_Vh']\n",
    "        index_key = your_dict['record_id']\n",
    "        better_bcr = self.__embedding_BCR(betterCDR_key,betterV_key,precise = True)\n",
    "        self.__get_antigen_in(antigen_key)\n",
    "        antigen_feat = self.antigen_in[antigen_key]\n",
    "#        print('antigen shape',self.antigen_in[antigen_key].shape)\n",
    "        better_pair = self.__comb_embed_gpu(antigen_key,better_bcr)\n",
    "        if any('Worse' in key for key in your_dict):\n",
    "            worseCDR_key = your_dict['WorseBCR_CDR3h']\n",
    "            worseV_key = your_dict['WorseBCR_Vh']\n",
    "            worse_bcr = self.__embedding_BCR(worseCDR_key,worseV_key,precise = True)\n",
    "            worse_pair = self.__comb_embed_gpu(antigen_key,worse_bcr)\n",
    "            return better_pair, worse_pair, index_key, antigen_key#better_feat, worse_feat,\n",
    "        else:\n",
    "            score = your_dict['Score']\n",
    "            return better_pair, score, index_key, antigen_key#better_feat, worse_feat,\n",
    "\n",
    "#        self.lens_dict[antigen_key] = aalens_key\n",
    "#        antigen_feat = self.__extract_antigen(antigen_key)\n",
    "        ##check whether antigen_index in antigen_in; if not, import it to\n",
    "\n",
    "\n",
    "#        print('better shape',better_feat.shape)\n",
    "#         better_pair = self.__comb_embed(antigen_key,better_feat)\n",
    "#         worse_pair = self.__comb_embed(antigen_key,worse_feat)\n",
    "\n",
    "\n",
    "#        better_out = better_pair.squeeze(dim=0)\n",
    "#        worse_out = worse_pair.squeeze(dim=0)\n",
    "        # print('better out dtype:',better_out.dtype)\n",
    "        # print('worse out dtype:',worse_out.dtype)\n",
    "#        out_dict = {}\n",
    "#        out_dict[index_key] = (better_out, worse_out)\n",
    "#        return better_out.to(device), worse_out.to(device)#, aalen_key #IF RUN COMBINING_EMBED in cpu\n",
    "    #IF RUN COMBINING_EMBED in GPU:\n",
    "\n",
    "    def get_my_data_list(self,selected_cols = 'BCR_Vh|BCR_CDR3h|Antigen|aalens|antigen_index|record_id|Score'):\n",
    "        ds_to_dict = self.dataframe.filter(regex=selected_cols)\n",
    "    #ds_to_dict = dataset[selected_cols].set_index('record_id')\n",
    "        my_data_list = ds_to_dict.to_dict(orient='records')\n",
    "        return my_data_list\n",
    "\n",
    "    def generate_group_index(self, group_size):\n",
    "        df = self.dataframe.copy()\n",
    "        groups = df.groupby('antigen_index').cumcount() // group_size + 1\n",
    "        df['group'] = groups\n",
    "        df['group_id'] = df.apply(lambda row: row['antigen_index'] + '_' + str(row['group']), axis=1)\n",
    "        group_id = df['group_id'].tolist()\n",
    "        return group_id\n",
    "\n",
    "    def __comb_embed_gpu(self,antigen_name,BCR_feat):\n",
    "#        print('antigen to comb:',antigen_name)\n",
    "        \n",
    "        #rint('The current antigen is: ',antigen_name)\n",
    "#        print('length get from dict_len:',lengthen)\n",
    "        single_antigen_g = self.antigen_in[antigen_name][0]\n",
    "    \n",
    "        lengthen = single_antigen_g.shape[1]\n",
    "        \n",
    "#        single_antigen_g = F.normalize(single_antigen_g, p=2, dim=3)\n",
    "#        print('shape of antigen from antigen dict:',single_antigen_g.shape)\n",
    "#        single_antigen_g = torch.from_numpy(single_antigen).to(device)\n",
    "        single_BCR_g = torch.from_numpy(BCR_feat).to(device)\n",
    "#        print('single BCR shape:',single_BCR_g.shape)\n",
    "#        single_BCR_g = torch.from_numpy(BCR_feat).half().to(device)\n",
    "        BCR_t = torch.tile(single_BCR_g,(lengthen,lengthen,1))\n",
    "#        print('tiled bcr shape',BCR_t.shape)\n",
    "#        print('shape of BCR_tiled:',BCR_t.shape)#empty\n",
    "        pair_feat_g = torch.cat((single_antigen_g,BCR_t),dim=2)\n",
    "        del single_BCR_g,BCR_t\n",
    "        torch.cuda.empty_cache()\n",
    "        return pair_feat_g#.half()\n",
    "#     def __comb_embed(self,single_antigen,BCR_feat):\n",
    "#         '''process your data'''\n",
    "#     #    singe_antigen = antigen_dict[antigen_name]\n",
    "#         lengthen = aalen(single_antigen)\n",
    "# #         if verbose ==True:\n",
    "# #             if lengthen > 500:\n",
    "# #                 print(Fore.RED + 'length of antigen over 500: '+str(lengthen))\n",
    "# #                 print(Style.RESET_ALL)\n",
    "#         BCR_tile = np.tile(BCR_feat,(1,lengthen,lengthen,1))\n",
    "#         pair_np = np.concatenate((single_antigen,BCR_tile),axis=3)\n",
    "#         pair_feat = torch.from_numpy(pair_np)\n",
    "#         return(pair_feat)\n",
    "\n",
    "    def __get_antigen_in(self,antigen_name):\n",
    "        #print('Next antigen:',antigen_name)\n",
    "        if not antigen_name in self.antigen_in:\n",
    "            if not antigen_name in self.antigen_dict:\n",
    "                antigen_to_in = self.extract_antigen(antigen_name)\n",
    "            else:\n",
    "                antigen_to_in = self.antigen_dict[antigen_name]\n",
    "            ##check and import\n",
    "            self.__rotate_dict_in(antigen_name,limit_size = LIMIT)\n",
    "            #single_antigen_in =\n",
    "            try:\n",
    "\n",
    "                antigen_tensor = torch.from_numpy(antigen_to_in).to(device)\n",
    "                self.antigen_in[antigen_name] = self.pool_antigen(antigen_tensor,CHANNEL_ANTIGEN) ###ON CPU\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(\"CUDA out of memory. Clearing cache and trying again...\")\n",
    "                    for key in self.antigen_in:\n",
    "                        self.antigen_in.pop(key, None)\n",
    "                    self.antigen_in.clear()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # Try the operation again\n",
    "                    try:\n",
    "                        self.antigen_in[antigen_name] = torch.from_numpy(antigen_to_in).to(device)\n",
    "                    except RuntimeError as e:\n",
    "                        if \"CUDA out of memory\" in str(e):\n",
    "                            print(\"Still out of memory after clearing cache.\")\n",
    "                        else:\n",
    "                            raise\n",
    "                else:\n",
    "                    raise\n",
    "#            self.antigen_in[antigen_name] = torch.from_numpy(antigen_to_in).half().to(device)\n",
    "\n",
    "#        return single_antigen_in\n",
    "    def __rotate_dict_in(self,antigen_name,limit_size=LIMIT,verbose=False):\n",
    "#        while self.__check_size(antigen_name,limit_size = LIMIT) and len(self.antigen_in)>0:\n",
    "        while self.__check_size(antigen_name,limit_size = LIMIT,datatype = 'float32') and len(self.antigen_in)>0 or len(self.antigen_in)> Max_num:\n",
    "\n",
    "            key = random.choice(list(self.antigen_in.keys()))\n",
    "#            print(self.antigen_in.keys())\n",
    "            self.antigen_in.pop(key, None)\n",
    "            if verbose:\n",
    "                print(Fore.RED +'the antigen:'+str(key)+'is deleted!'+ Fore.RESET)\n",
    "            torch.cuda.empty_cache()\n",
    "#        if len(self.antigen_in) >3:\n",
    "            if verbose:\n",
    "                print(Fore.RED +'antigens still in:'+str(self.antigen_in.keys())+ Fore.RESET)\n",
    "\n",
    "    def pool_antigen(self,antigen_input,out_n_channel):\n",
    "#        lengthen = antigen_input.shape[1]\n",
    "        #pooling_layer = nn.AdaptiveAvgPool2d((out_n_channel,out_n_channel))\n",
    "        #output = pooling_layer(antigen_input.permute(0,3,1,2)).permute(0,2,3,1)\n",
    "        output=antigen_input\n",
    "        return output\n",
    "    def __check_size(self,antigen_name,limit_size= LIMIT,datatype = 'float32'):\n",
    "        dict_size = 0\n",
    "        for key in self.antigen_in:\n",
    "    #        print(key,\":\")\n",
    "            aalen = self.lens_dict[key]\n",
    "    #        print(aalen)\n",
    "            dict_size += predict_size(aalen,datatype = datatype)\n",
    "    #    print('size in dict:',dict_size)\n",
    "        check = dict_size + predict_size(self.lens_dict[antigen_name])\n",
    "        check_limit = limit_size*(1024**3)\n",
    "    #    print('size all to check:',check_size)\n",
    "        return check > check_limit\n",
    "\n",
    "\n",
    "    def extract_antigen(self,antigen_name,verbose=False):\n",
    "        if antigen_name in self.antigen_dict:\n",
    "            single_antigen = self.antigen_dict[antigen_name]\n",
    "        else:\n",
    "            try:\n",
    "                antigen_import = np.load(str(self.antigen_fpath_dict+'/'+antigen_name+'.pair.npy'))/w\n",
    "                if not antigen_import.shape[1] == self.lens_dict[antigen_name]:\n",
    "                    print(Fore.RED + 'antigen ' +str(antigen_name)+' embedding '+str(antigen_import.shape[1])+' is NOT in the correct shape '+str(self.lens_dict[antigen_name])+'!'+ Style.RESET_ALL)\n",
    "                    exit()\n",
    "                single_antigen = antigen_import\n",
    "#                single_antigen = torch.from_numpy(antigen_import).to(device) ###ON GPU\n",
    "#            print(npy.shape)\n",
    "                self.antigen_dict[antigen_name] = single_antigen\n",
    "                if verbose:\n",
    "                    print(Fore.RED + 'New antigen added to dictionary:'+antigen_name+Fore.RESET)\n",
    "            except ValueError:\n",
    "                print('The embedding of antigen %s cannot be found!' % antigen_name)\n",
    "#             if verbose:\n",
    "#                 print(Fore.RED + 'New antigen added to dictionary:',antigen_name)\n",
    "#                 print(Fore.RED + 'Number of antigens included:',len(self.antigen_dict))\n",
    "#                 print(Style.RESET_ALL)\n",
    "        return single_antigen\n",
    "\n",
    "    def __embedding_BCR(self,cdr3_seq,v_seq,precise = False):\n",
    "        if cdr3_seq not in self.cdr3_dict:\n",
    "#            print('CDR3 not in dictionary!!')\n",
    "#            df1 = pd.DataFrame()\n",
    "            cdr3_feat,*_ = embedCDR3([cdr3_seq],precise = precise)\n",
    "            cdr3_feat = cdr3_feat[0]\n",
    "            self.cdr3_dict[cdr3_seq]=cdr3_feat\n",
    "        else:\n",
    "#            print('CDR3 in dictionary!!')\n",
    "            cdr3_feat = self.cdr3_dict[cdr3_seq]\n",
    "        if v_seq not in self.v_dict:\n",
    "#            print('V not in dictionary!!')\n",
    "#            df2 = pd.DataFrame([v_seq])\n",
    "            v_feat,*_ = embedV([v_seq],precise = precise)\n",
    "            v_feat = v_feat[0]\n",
    "            self.v_dict[v_seq]=v_feat\n",
    "        else:\n",
    "#            print('V in dictionary!!')\n",
    "            v_feat = self.v_dict[v_seq]\n",
    "        bcr_feat = np.concatenate((cdr3_feat,v_feat))\n",
    "        return bcr_feat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.your_data_list)\n",
    "\n",
    "\n",
    "class GroupShuffleSampler(Sampler):\n",
    "    def __init__(self, data_source, group_ids, shuffle=True):\n",
    "        self.data_source = data_source\n",
    "        self.group_ids = group_ids\n",
    "        self.group_indices = self._group_indices()\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def _group_indices(self):\n",
    "        \"\"\"b\n",
    "        Returns a dictionary where the keys are the unique group_ids and\n",
    "        the values are the indices corresponding to each group_id in the data_source.\n",
    "        \"\"\"\n",
    "        group_indices = {}\n",
    "        for idx, group_id in enumerate(self.group_ids):\n",
    "            if group_id not in group_indices:\n",
    "                group_indices[group_id] = []\n",
    "            group_indices[group_id].append(idx)\n",
    "        return group_indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Generates an iterator that yields indices of the data_source\n",
    "        \"\"\"\n",
    "        group_ids = list(self.group_indices.keys())\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(group_ids)\n",
    "        indices = []\n",
    "        for group_id in group_ids:\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.group_indices[group_id])\n",
    "            indices += self.group_indices[group_id]\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "class InLoader:\n",
    "    def __init__(self, dataframe, antigen_fpath_dict, cdr3_dict, v_dict,train_ratio=0.8, subsample_ratio=.01, seed=SEED):\n",
    "        self.dataframe = dataframe\n",
    "        self.train_ratio = train_ratio\n",
    "        self.subsample_ratio = subsample_ratio\n",
    "        self.seed = seed\n",
    "        self.antigen_fpath_dict = antigen_fpath_dict\n",
    "        self.cdr3_dict = cdr3_dict\n",
    "        self.v_dict = v_dict\n",
    "#        self.lens_dict = lens_dict\n",
    "        self.train_df, self.val_df = self.split_data()\n",
    "\n",
    "    def split_data(self):\n",
    "        train_df, val_df = train_test_split(\n",
    "            self.dataframe, train_size=self.train_ratio, random_state=self.seed)\n",
    "        return train_df, val_df\n",
    "\n",
    "    def get_train_dataset(self):\n",
    "        return SampleDataset(self.train_df, self.antigen_fpath_dict, self.cdr3_dict, self.v_dict, subsample_ratio=self.subsample_ratio)\n",
    "\n",
    "    def get_val_dataset(self):\n",
    "        return SampleDataset(self.val_df, self.antigen_fpath_dict, self.cdr3_dict, self.v_dict, subsample_ratio=self.subsample_ratio)\n",
    "\n",
    "    def new_epoch(self):\n",
    "#        print(\"Starting a new epoch...\")\n",
    "        self.train_df, self.val_df = self.split_data()\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of SelfAttentionPooling\n",
    "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
    "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim,hidden_dim):\n",
    "        super(SelfAttentionPooling, self).__init__()\n",
    "#        hidden_dim=10\n",
    "        self.W1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.W2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "\n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "\n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "        att_w = F.softmax(self.W2(self.relu(self.W1(batch_rep))).squeeze(-1),dim=1).unsqueeze(-1)\n",
    "\n",
    "        utter_rep = torch.sum(batch_rep* att_w, dim=1)\n",
    "\n",
    "        return utter_rep\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "###MODEL\n",
    "\n",
    "class mix_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mix_model,self).__init__()\n",
    "        \n",
    "        self.model10 = nn.Sequential(\n",
    "            nn.Linear(318,40),#.to(torch.float64),\n",
    "            # in (1,len,len,318)\n",
    "            # out (1,len,len.50)\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        \n",
    "        self.model11 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((CHANNEL_ANTIGEN,CHANNEL_ANTIGEN))\n",
    "        )\n",
    "        \n",
    "        self.model12 = nn.Sequential(\n",
    "            nn.Linear(40,30),#.to(torch.float64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(30,20),#.to(torch.float64),\n",
    "            # out (1,len,len,20)\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.model2 = SelfAttentionPooling(input_dim=20,hidden_dim=30)\n",
    "        self.model2_1 = SelfAttentionPooling(input_dim=20,hidden_dim=30)\n",
    "        # input_dim = hidden size (number of channels)\n",
    "#        self.model2 = nn.MultiheadAttention(embed_dim=20,num_heads=N_HEADS,batch_first=True)\n",
    "        self.model3 = nn.Sequential(\n",
    "            nn.Linear(20,15),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(15,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x,binary = True, is_10X = True): ###because in getitem, return is .cuda(), now input is on gpu\n",
    "#         x = torch.empty(0)\n",
    "#         x = x.to(device)\n",
    "#        x = x.permute(0,2,1,3)\n",
    "#         print('after permute',x.shape)\n",
    "        x = self.model10(x)\n",
    "#         print('after model1',x.shape)\n",
    "        x0 = torch.empty(0)\n",
    "        x0 = x0.to(device)\n",
    "        for i in range(len(x)):\n",
    "            k = x[i]\n",
    "            k=k.permute(2,0,1)\n",
    "            k=self.model11(k)\n",
    "            k=k.permute(1,2,0)\n",
    "            k=self.model12(k)\n",
    "            k = self.model2(k).unsqueeze(0)\n",
    "#             print('after model2',k.shape)\n",
    "            k = self.model2_1(k)\n",
    "#             print('after model2_1',k.shape)\n",
    "            x0 = torch.cat((x0, k), dim=0)\n",
    "#         print('after loop:',x0.shape)\n",
    "        x0 = F.normalize(x0)\n",
    "        x0 = self.model3(x0).squeeze()\n",
    "        if binary:\n",
    "            out  = x0\n",
    "        else:\n",
    "            if is_10X:\n",
    "                out = x0*(-math.exp(self.alpha1))+self.beta1\n",
    "            else:\n",
    "                out = x0*(-math.exp(self.alpha2))+self.beta2\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e798e6e3-c303-4635-b93e-8f0a744a5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(out_b,out_w,binary = True):\n",
    "    if not binary:\n",
    "        print('wrong')\n",
    "    else:\n",
    "        loss = torch.sum(torch.relu(out_b-out_w+ T)+LAMBDA*(out_b*out_b + out_w*out_w))\n",
    "    return loss/out_b.numel()\n",
    "\n",
    "\n",
    "\n",
    "def batch_result(batch,model,loss_fn,binary = True,is_10X=True):\n",
    "#    print(zip(index_idx,antigen_index))\n",
    "#    print(b_pair.shape,w_pair.shape)\n",
    "    b_pair,w_pair,index_idx,antigen_index =batch ##Change the InLoader, when binary is False, w_pair = score\n",
    "    out_b = model(b_pair,binary = binary, is_10X = is_10X)\n",
    "    if binary:\n",
    "        out_w = model(w_pair)\n",
    "        loss = loss_fn(out_b,out_w,binary = binary)\n",
    "        outb = out_b.tolist()\n",
    "        outw = out_w.tolist()\n",
    "        success = torch.gt(out_w,out_b).tolist()\n",
    "        df = pd.DataFrame({'record_id':index_idx,'antigen':antigen_index,'out_b':outb,'out_w':outw,'success':success})\n",
    "    else:\n",
    "#         b_pair,score,index_idx,antigen_index =batch ##Change the InLoader, when binary is False, w_pair = score\n",
    "#         predict = model(b_pair,binary = bin)\n",
    "        out_score = w_pair.to(device)\n",
    "        loss = loss_fn(out_b,out_score,binary = binary)\n",
    "        pred = out_b.tolist()\n",
    "        score = out_score.tolist()\n",
    "        df = pd.DataFrame({'record_id':index_idx,'antigen':antigen_index,'predicted':pred,'score':w_pair,'success':np.nan})\n",
    "    return(df,loss)\n",
    "\n",
    "\n",
    "def train_epoch(dataloader,model,loss_fn,optimizer,clip_value,verbose = False,binary = True, is_10X = True):\n",
    "    train_loss = 0.0\n",
    "    i = 0\n",
    "    res = pd.DataFrame(columns=['record_id', 'antigen', 'out_b','out_w','success'])\n",
    "#    model = select_model(model_class,device=device,binary = binary, is_10X = is_10X)\n",
    "#     optimizer,scheduler = generate_optimizer(model,lr=LR)\n",
    "    model.train()\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        df,loss=batch_result(batch,model,loss_fn,binary = binary,is_10X=is_10X) ###MARK HERE\n",
    "        res = pd.concat([res,df],axis=0,ignore_index=True)\n",
    "        train_loss += loss.item()\n",
    "#         if binary:\n",
    "#             accu_biased = sum(res['success'])/res.shape[0]\n",
    "#             grouped = res.groupby('antigen')['success'].mean()\n",
    "#             accu_unbiased = grouped.mean()\n",
    "#             if verbose:\n",
    "#                 print('Batch',i,'batch_loss',loss.item(),'loss_until_now',train_loss/i,'accu_biased',accu_biased,'accu_unbiased',accu_unbiased)\n",
    "#         else:\n",
    "        if verbose:\n",
    "            print('Batch',i,'batch_loss',loss.item(),'loss_until_now',train_loss/i)\n",
    "        i += 1\n",
    "        if math.isinf(loss) or math.isnan(loss):\n",
    "            prob_bw = [out_b.item(),out_w.item()]\n",
    "            print('ERROR: The loss is INF or NaN! '+str(prob_bw))\n",
    "            break\n",
    "        loss.backward()\n",
    "        if clip_value is not None:\n",
    "            nn.utils.clip_grad_value_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "#     return train_loss/i,accu_biased,accu_unbiased,res,grouped\n",
    "    return train_loss/i,res\n",
    "\n",
    "def val_epoch(dataloader,model,loss_fn,verbose = False):\n",
    "    val_loss = 0.0\n",
    "    i = 0\n",
    "    res = pd.DataFrame(columns=['record_id', 'antigen', 'out_b','out_w','success'])\n",
    "#    model = select_model(model_class,device=device,binary = True)\n",
    "#    optimizer,scheduler = generate_optimizer(model,lr=LR)\n",
    "    model.eval()\n",
    "    for batch in tqdm(dataloader):\n",
    "        df,loss=batch_result(batch,model,loss_fn)\n",
    "        res = pd.concat([res,df],axis=0,ignore_index=True)\n",
    "        val_loss += loss.item()\n",
    "        accu_biased = sum(res['success'])/res.shape[0]\n",
    "        grouped = res.groupby('antigen')['success'].mean()\n",
    "        accu_unbiased = grouped.mean()\n",
    "        i += 1\n",
    "        if verbose:\n",
    "            print('Batch',i,'batch_loss',loss.item(),'loss_until_now',val_loss/i,'accu_biased',accu_biased,'accu_unbiased',accu_unbiased)\n",
    "        if math.isinf(loss) or math.isnan(loss):\n",
    "            prob_bw = [out_b.tolist(),out_w.tolist()]\n",
    "            print('ERROR: The loss is INF or NaN! '+str(prob_bw))\n",
    "            break\n",
    "    return val_loss/i,accu_biased,accu_unbiased,res,grouped\n",
    "\n",
    "\n",
    "\n",
    "# Usualy Small_sample is None, we use this function small_data to make each antigen has similar number of entries.\n",
    "if Small_sample is not None:\n",
    "#    input_df = small_data_in(input_ori,each = 3, around =1)\n",
    "#    exVal = exVal_ori.sample(n = Small_sample)\n",
    "    input_df = small_data(input_ori,subsample=Small_sample,each = EACH)\n",
    "    cont_train_10x =  small_data(cont_train_10x,subsample=Small_sample,each = EACH)\n",
    "    cont_train_libra =  small_data(cont_train_libra,subsample=Small_sample,each = EACH)\n",
    "    exVal = small_data(exVal_ori,subsample=Small_sample,each = EACH)\n",
    "    cont_val =  small_data(cont_val_ori,subsample=cSmall_sample,each = EACH)\n",
    "else:\n",
    "    input_df = input_ori\n",
    "    cont_train_10x = cont_train_10x\n",
    "    cont_train_libra = cont_train_libra\n",
    "    exVal = small_data(exVal_ori,subsample=exVal_ori.shape[0],each = EACH)\n",
    "    cont_val =  small_data(cont_val_ori,subsample=cont_val_ori.shape[0],each = EACH)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee01e378-1eac-43b6-ab7b-0f8b2ed8af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing antigens larger than 1800, 99.77132466387481% antigens remained.\n",
      "3diff_easy_negative_3rep Data:\n",
      "\u001b[32mBetterBCR_Vh PASS!\n",
      "\u001b[32mBetterBCR_CDR3h PASS!\n",
      "\u001b[32mWorseBCR_Vh PASS!\n",
      "\u001b[32mWorseBCR_CDR3h PASS!\n",
      "\u001b[32mAntigen PASS!\n",
      "\u001b[0m\n",
      "After removing antigens larger than 1800, 100.0% antigens remained.\n",
      "External Validation Data:\n",
      "\u001b[32mBetterBCR_Vh PASS!\n",
      "\u001b[32mBetterBCR_CDR3h PASS!\n",
      "\u001b[32mWorseBCR_Vh PASS!\n",
      "\u001b[32mWorseBCR_CDR3h PASS!\n",
      "\u001b[32mAntigen PASS!\n",
      "\u001b[0m\n",
      "After removing antigens larger than 1800, 100.0% antigens remained.\n",
      "Continuous 10X training Data:\n",
      "\u001b[32mBetterBCR_Vh PASS!\n",
      "\u001b[32mBetterBCR_CDR3h PASS!\n",
      "\u001b[32mAntigen PASS!\n",
      "\u001b[0m\n",
      "After removing antigens larger than 1800, 100.0% antigens remained.\n",
      "Continuous Libra training Data:\n",
      "\u001b[32mBetterBCR_Vh PASS!\n",
      "\u001b[32mBetterBCR_CDR3h PASS!\n",
      "\u001b[32mAntigen PASS!\n",
      "\u001b[0m\n",
      "After removing antigens larger than 1800, 100.0% antigens remained.\n",
      "External Continuous Validation Data:\n",
      "\u001b[32mBetterBCR_Vh PASS!\n",
      "\u001b[32mBetterBCR_CDR3h PASS!\n",
      "\u001b[32mWorseBCR_Vh PASS!\n",
      "\u001b[32mWorseBCR_CDR3h PASS!\n",
      "\u001b[32mAntigen PASS!\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# In[34]:\n",
    "\n",
    "# Preprocessing...\n",
    "\n",
    "input_df = preprocess(input_df,CUTOFF)\n",
    "print(INPUT,'Data:')\n",
    "check_bad_letters(input_df)\n",
    "\n",
    "exVal = preprocess(exVal,CUTOFF)\n",
    "print('External Validation Data:')\n",
    "check_bad_letters(exVal)\n",
    "\n",
    "c_10x = preprocess(cont_train_10x,CUTOFF,binary = False)\n",
    "print('Continuous 10X training Data:')\n",
    "check_bad_letters(c_10x,binary = False)\n",
    "\n",
    "c_libra = preprocess(cont_train_libra,CUTOFF,binary = False)\n",
    "print('Continuous Libra training Data:')\n",
    "check_bad_letters(c_libra,binary = False)\n",
    "\n",
    "cVal = preprocess(cont_val,CUTOFF)\n",
    "print('External Continuous Validation Data:')\n",
    "check_bad_letters(cVal)\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "exVal = exVal.reindex(columns=input_df.columns)\n",
    "input_cb = pd.concat([input_df, exVal,c_10x,c_libra,cVal], axis=0)\n",
    "\n",
    "aalens_df = input_cb.groupby('antigen_index')['aalens'].mean().astype(int)\n",
    "len_dict = aalens_df.to_dict()\n",
    "\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "#len_dict['Hie/EbolaGP']\n",
    "\n",
    "\n",
    "# In[49]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1df71ef-500e-4a9c-8ae6-ff4776ddbe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_loader = InLoader(input_df, NPY_DIR,CDR3h_dict,Vh_dict,train_ratio=0.9, subsample_ratio=0.001, seed=SEED)\n",
    "cont_loader_10x = InLoader(c_10x, NPY_DIR,CDR3h_dict,Vh_dict,train_ratio=0.995, subsample_ratio=1, seed=SEED)\n",
    "cont_loader_lib = InLoader(c_libra, NPY_DIR,CDR3h_dict,Vh_dict,train_ratio=0.995, subsample_ratio=1, seed=SEED)\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "# Get the train and validation datasets\n",
    "b_val_dataset = SampleDataset(exVal, NPY_DIR, CDR3h_hard_dict, Vh_hard_dict, subsample_ratio=EX_SUBSAMPLE)\n",
    "b_ex_loader = DataLoader(b_val_dataset, 1, sampler=GroupShuffleSampler(b_val_dataset,b_val_dataset.group_ids))\n",
    "\n",
    "c_val_dataset = SampleDataset(cVal, NPY_DIR, CDR3h_hard_dict, Vh_hard_dict, subsample_ratio=EX_SUBSAMPLE)\n",
    "c_ex_loader = DataLoader(c_val_dataset, 1, sampler=GroupShuffleSampler(c_val_dataset,c_val_dataset.group_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f54425-9115-47d6-97d7-5a2bcb37afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/DPDS/Wang_lab/shared/BCR_antigen/data/output/tw_tag1 is created!\n",
      "/project/DPDS/Wang_lab/shared/BCR_antigen/data/output/tw_tag1/model_comb is created!\n",
      "/project/DPDS/Wang_lab/shared/BCR_antigen/data/output/tw_tag1/process_res is created!\n"
     ]
    }
   ],
   "source": [
    "# In[48]:\n",
    "\n",
    "\n",
    "parent_dir = '/project/DPDS/Wang_lab/shared/BCR_antigen/data/output'\n",
    "out_dir = os.path.join(parent_dir,'tw_tag'+str(TAG))\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass\n",
    "print(out_dir,'is created!')\n",
    "model_dir = out_dir +'/model_comb'\n",
    "try:\n",
    "    os.mkdir(model_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "print(model_dir,'is created!')\n",
    "res_dir = out_dir +'/process_res'\n",
    "try:\n",
    "    os.mkdir(res_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "print(res_dir,'is created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36bd9301-4acc-4508-9d10-ae543f01df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mix = mix_model()#.half()\n",
    "if MODEL is not None:\n",
    "    checkpoint = torch.load('/project/DPDS/Wang_lab/shared/BCR_antigen/data/output/'+ MODEL)\n",
    "    model_mix.load_state_dict(checkpoint)\n",
    "model_mix.to(device)\n",
    "optimizer = torch.optim.Adam(model_mix.parameters(),lr=LR)\n",
    "scheduler = ReduceLROnPlateau(optimizer,mode = 'min',factor=0.1,patience =5,verbose=False,threshold_mode='rel',threshold=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50796e-2531-49b6-8c99-f9e5fa9c10bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "epoch: 0\n",
      "binary training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:23<00:00,  4.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 binary train loss: 0.004711404737389209binary val loss: 0.005226423455910249binary val accu biased: 0.6363636363636364binary val accuracy: 0.6333333333333333\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████▌            | 744/849 [02:32<00:29,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mantigen Hie/EbolaGP embedding 495 is NOT in the correct shape 230!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [02:45<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 binary external val loss: 0.02402855244929188 binary ex accu biased: 0.7220259128386337 binary external val accuracy: 0.6924633711498823\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:41<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cont external val loss: 0.004418350132035959 cont ex accu biased: 0.6335078534031413 cont external val accuracy: 0.5866272364201526\n",
      "continuous Libra-seq training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [01:28<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cont libra train loss: 0.005051255776567798\n",
      "continuous 10X training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4717/4717 [04:15<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cont 10x train loss: 0.00486504175882484\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:58<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 binary external val loss: 0.004348203357752277 binary ex accu biased: 0.7326266195524146 binary external val accuracy: 0.7052655341527236\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:42<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cont external val loss: 0.004868779612630346 cont ex accu biased: 0.5951134380453752 cont external val accuracy: 0.5351905648375663\n",
      "epoch: 1\n",
      "binary training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:22<00:00,  4.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 binary train loss: 0.004912849534683081binary val loss: 0.0051274210722609binary val accu biased: 0.7272727272727273binary val accuracy: 0.7321428571428572\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:58<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 binary external val loss: 0.004671733819817065 binary ex accu biased: 0.6277974087161367 binary external val accuracy: 0.6235334013584803\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:33<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 cont external val loss: 0.005084117593318484 cont ex accu biased: 0.47643979057591623 cont external val accuracy: 0.4599232664849755\n",
      "continuous Libra-seq training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:56<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 cont libra train loss: 0.005023001669964142\n",
      "continuous 10X training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4717/4717 [02:54<00:00, 27.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 cont 10x train loss: 0.004726688113977325\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:57<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 binary external val loss: 0.005322274444811265 binary ex accu biased: 0.7126030624263839 binary external val accuracy: 0.7122298635092328\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:50<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 cont external val loss: 0.00490549823226989 cont ex accu biased: 0.5235602094240838 cont external val accuracy: 0.49114738292196075\n",
      "epoch: 2\n",
      "binary training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:25<00:00,  3.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 binary train loss: 0.004947557855321437binary val loss: 0.006382337178696285binary val accu biased: 0.45454545454545453binary val accuracy: 0.48214285714285715\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [02:06<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 binary external val loss: 0.00405966838354462 binary ex accu biased: 0.6513545347467609 binary external val accuracy: 0.633601914771865\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:50<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 cont external val loss: 0.004920591366189625 cont ex accu biased: 0.5584642233856894 cont external val accuracy: 0.582335620993901\n",
      "continuous Libra-seq training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [01:01<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 cont libra train loss: 0.004932973577996293\n",
      "continuous 10X training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4717/4717 [02:50<00:00, 27.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 cont 10x train loss: 0.004662097373672062\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:55<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 binary external val loss: 0.014238226591654394 binary ex accu biased: 0.7338044758539458 binary external val accuracy: 0.7188410638373915\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:38<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 cont external val loss: 0.004284480757309192 cont ex accu biased: 0.6963350785340314 cont external val accuracy: 0.6172704985108399\n",
      "epoch: 3\n",
      "binary training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:24<00:00,  4.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 binary train loss: 0.005544795265079153binary val loss: 0.004913280548697168binary val accu biased: 0.7272727272727273binary val accuracy: 0.7301587301587301\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:49<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 binary external val loss: 0.004106079277585758 binary ex accu biased: 0.7279151943462897 binary external val accuracy: 0.6899650194646997\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:45<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 cont external val loss: 0.004929787697137234 cont ex accu biased: 0.5671902268760908 cont external val accuracy: 0.4823219831266773\n",
      "continuous Libra-seq training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [01:03<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 cont libra train loss: 0.004932208574305002\n",
      "continuous 10X training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4717/4717 [02:47<00:00, 28.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 cont 10x train loss: 0.004611190483692733\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:50<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 binary external val loss: 0.010911254976844267 binary ex accu biased: 0.6065959952885748 binary external val accuracy: 0.6230099407735901\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:46<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 cont external val loss: 0.004700463751158685 cont ex accu biased: 0.6160558464223386 cont external val accuracy: 0.5684489067573375\n",
      "epoch: 4\n",
      "binary training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:17<00:00,  5.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 binary train loss: 0.004937802099299674binary val loss: 0.004889183593067256binary val accu biased: 0.45454545454545453binary val accuracy: 0.5499999999999999\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:37<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 binary external val loss: 0.0063193832426491 binary ex accu biased: 0.6525323910482921 binary external val accuracy: 0.648298497345966\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:36<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 cont external val loss: 0.00458485332933656 cont ex accu biased: 0.7015706806282722 cont external val accuracy: 0.7341150306225365\n",
      "continuous Libra-seq training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:53<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 cont libra train loss: 0.004848447433229248\n",
      "continuous 10X training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4717/4717 [02:46<00:00, 28.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 cont 10x train loss: 0.00454344635938711\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:39<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 binary external val loss: 0.014412676333038419 binary ex accu biased: 0.6819787985865724 binary external val accuracy: 0.6745444781772885\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:32<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 cont external val loss: 0.004412724924181145 cont ex accu biased: 0.699825479930192 cont external val accuracy: 0.6940049400323158\n",
      "epoch: 5\n",
      "binary training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:23<00:00,  4.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 binary train loss: 0.004381765754019119binary val loss: 0.005721962926062671binary val accu biased: 0.5454545454545454binary val accuracy: 0.5185185185185185\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:40<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 binary external val loss: 0.009123904184077387 binary ex accu biased: 0.7149587750294464 binary external val accuracy: 0.6925258680126157\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:34<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 cont external val loss: 0.004136294217529929 cont ex accu biased: 0.7574171029668412 cont external val accuracy: 0.7813330520351798\n",
      "continuous Libra-seq training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:55<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 cont libra train loss: 0.004863689165850381\n",
      "continuous 10X training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4717/4717 [02:38<00:00, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 cont 10x train loss: 0.004542442760494444\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:38<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 binary external val loss: 0.011470063872530056 binary ex accu biased: 0.7243816254416962 binary external val accuracy: 0.7267891128670239\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:27<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 cont external val loss: 0.004688100408223079 cont ex accu biased: 0.631762652705061 cont external val accuracy: 0.5664261612834769\n",
      "epoch: 6\n",
      "binary training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:06<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 binary train loss: 0.005071786156266319binary val loss: 0.004729524424130266binary val accu biased: 0.45454545454545453binary val accuracy: 0.41666666666666663\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [02:04<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 binary external val loss: 0.0054478389161635625 binary ex accu biased: 0.7126030624263839 binary external val accuracy: 0.7158551134518504\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:38<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 cont external val loss: 0.004764999682327528 cont ex accu biased: 0.7068062827225131 cont external val accuracy: 0.7279331449925179\n",
      "continuous Libra-seq training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [01:10<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 cont libra train loss: 0.004765870079788335\n",
      "continuous 10X training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4717/4717 [03:12<00:00, 24.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 cont 10x train loss: 0.004471780332063994\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [02:04<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 binary external val loss: 0.015606723267264725 binary ex accu biased: 0.558303886925795 binary external val accuracy: 0.5734791063776059\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:49<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 cont external val loss: 0.0040882861690293435 cont ex accu biased: 0.6387434554973822 cont external val accuracy: 0.6969723726817393\n",
      "epoch: 7\n",
      "binary training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:27<00:00,  3.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 binary train loss: 0.00473860663608933binary val loss: 0.004437331618233161binary val accu biased: 0.5454545454545454binary val accuracy: 0.34920634920634924\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [01:58<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 binary external val loss: 0.007550774755349078 binary ex accu biased: 0.6419316843345112 binary external val accuracy: 0.6207961162407438\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:47<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 cont external val loss: 0.004106829558900618 cont ex accu biased: 0.7155322862129145 cont external val accuracy: 0.7524566086478317\n",
      "continuous Libra-seq training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [01:03<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 cont libra train loss: 0.004747296739230281\n",
      "continuous 10X training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4717/4717 [03:10<00:00, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 cont 10x train loss: 0.004395557811710007\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [02:05<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 binary external val loss: 0.018362947940405182 binary ex accu biased: 0.6419316843345112 binary external val accuracy: 0.6446553238243397\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:50<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 cont external val loss: 0.00441867158297989 cont ex accu biased: 0.6492146596858639 cont external val accuracy: 0.6828811846407653\n",
      "epoch: 8\n",
      "binary training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:21<00:00,  4.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 binary train loss: 0.004017368225114686binary val loss: 0.004337450489401817binary val accu biased: 0.36363636363636365binary val accuracy: 0.2916666666666667\n",
      "binary validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [02:05<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 binary external val loss: 0.014642608364072088 binary ex accu biased: 0.6843345111896348 binary external val accuracy: 0.68102174791641\n",
      "continuous validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 573/573 [01:51<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 cont external val loss: 0.00441803006751115 cont ex accu biased: 0.6701570680628273 cont external val accuracy: 0.6011484587065934\n",
      "continuous Libra-seq training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████████████▊                                                         | 174/401 [00:17<00:07, 28.93it/s]"
     ]
    }
   ],
   "source": [
    "# In[40]:\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print('Start Training...')\n",
    "#MARK HERE\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:',str(epoch))\n",
    "    start_epoch_time = time.time()\n",
    "       \n",
    "\n",
    "    binary_loader.new_epoch()\n",
    "    print('binary training...')\n",
    "    b_train_loader,b_val_loader = gen_train_val_loader(binary_loader)\n",
    "#     train_dataset = inputloader.get_train_dataset()\n",
    "#     val_dataset = inputloader.get_val_dataset()\n",
    "#     train_loader = DataLoader(train_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(train_dataset, train_dataset.group_ids))\n",
    "#     val_loader = DataLoader(val_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(val_dataset, val_dataset.group_ids))\n",
    "    b_train_loss,*_ = train_epoch(b_train_loader,model_mix,loss_function,optimizer,clip_value=CLIP,verbose = VERBOSE)\n",
    "    b_val_loss,b_val_accu_biased,b_val_accuracy,*_ = val_epoch(b_val_loader,model_mix,loss_function,verbose = VERBOSE)\n",
    "    print('Epoch '+str(epoch)+' binary train loss: '+str(b_train_loss)+'binary val loss: '+str(b_val_loss)+'binary val accu biased: '+str(b_val_accu_biased)+'binary val accuracy: '+str(b_val_accuracy))\n",
    "\n",
    "    print('binary validating...')\n",
    "    b_ex_loss,b_ex_accu_biased,b_ex_accuracy,res_ex_b,group_ex_b = val_epoch(b_ex_loader,model_mix,loss_function,verbose=VERBOSE)\n",
    "    print('Epoch '+str(epoch)+ ' binary external val loss: '+str(b_ex_loss)+' binary ex accu biased: '+str(b_ex_accu_biased)+' binary external val accuracy: '+str(b_ex_accuracy))\n",
    "\n",
    "    print('continuous validating...')\n",
    "    c_ex_loss,c_ex_accu_biased,c_ex_accuracy,res_ex_c,group_ex_c = val_epoch(c_ex_loader,model_mix,loss_function,verbose=VERBOSE)\n",
    "    print('Epoch '+str(epoch)+ ' cont external val loss: '+str(c_ex_loss)+' cont ex accu biased: '+str(c_ex_accu_biased)+' cont external val accuracy: '+str(c_ex_accuracy))\n",
    "\n",
    "    \n",
    "#    if epoch%5 == 0:\n",
    "    res_ex_b.to_csv(res_dir+'/binary_ex_accuracy_tag'+str(TAG)+'_Epoch'+str(epoch)+'_Lr'+str(LR)+'_all.csv')\n",
    "    group_ex_b.to_csv(res_dir+'/binary_ex_accuracy_tag'+str(TAG)+'_Epoch'+str(epoch)+'_Lr'+str(LR)+'_per_antigen.csv')\n",
    "    res_ex_c.to_csv(res_dir+'/cont_ex_accuracy_tag'+str(TAG)+'_Epoch'+str(epoch)+'_Lr'+str(LR)+'_all.csv')\n",
    "    group_ex_c.to_csv(res_dir+'/cont_ex_accuracy_tag'+str(TAG)+'_Epoch'+str(epoch)+'_Lr'+str(LR)+'_per_antigen.csv')\n",
    "\n",
    "    print('continuous Libra-seq training...')\n",
    "    cont_loader_lib.new_epoch()\n",
    "    clib_train_loader = gen_train_val_loader(cont_loader_lib,validate=False)#,binary=False)\n",
    "    clib_train_loss,*_ = train_epoch(clib_train_loader,model_mix,loss_function,optimizer,clip_value=CLIP,verbose = VERBOSE)#,binary=False,is_10X=False)\n",
    "#     clib_val_loss,clib_val_accu_biased,clib_val_accuracy,*_ = val_epoch(clib_val_loader,model_mix,loss_function,verbose = VERBOSE)\n",
    "    print('Epoch '+str(epoch)+' cont libra train loss: '+str(clib_train_loss))#+'clib val loss: '+str(clib_val_loss)+'clib val accu biased: '+str(clib_val_accu_biased)+'clib val accuracy: '+str(clib_val_accuracy))\n",
    "\n",
    "    \n",
    "    print('continuous 10X training...')\n",
    "    cont_loader_10x.new_epoch()\n",
    "    c10_train_loader = gen_train_val_loader(cont_loader_10x,validate=False)#,binary=False)\n",
    "    c10_train_loss,*_ = train_epoch(c10_train_loader,model_mix,loss_function,optimizer,clip_value=CLIP,verbose = VERBOSE)#,binary=False)\n",
    "#     c10_val_loss,c10_val_accu_biased,c10_val_accuracy,*_ = val_epoch(c10_val_loader,model_mix,loss_function,verbose = VERBOSE)\n",
    "    print('Epoch '+str(epoch)+' cont 10x train loss: '+str(c10_train_loss))#+'c10 val loss: '+str(c10_val_loss)+'c10 val accu biased: '+str(c10_val_accu_biased)+'c10 val accuracy: '+str(c10_val_accuracy))\n",
    "\n",
    "    \n",
    "    print('binary validating...')\n",
    "    b_ex_loss,b_ex_accu_biased,b_ex_accuracy,res_ex_b,group_ex_b = val_epoch(b_ex_loader,model_mix,loss_function,verbose=VERBOSE)\n",
    "    print('Epoch '+str(epoch)+ ' binary external val loss: '+str(b_ex_loss)+' binary ex accu biased: '+str(b_ex_accu_biased)+' binary external val accuracy: '+str(b_ex_accuracy))\n",
    "\n",
    "    print('continuous validating...')\n",
    "    c_ex_loss,c_ex_accu_biased,c_ex_accuracy,res_ex_c,group_ex_c = val_epoch(c_ex_loader,model_mix,loss_function,verbose=VERBOSE)\n",
    "    print('Epoch '+str(epoch)+ ' cont external val loss: '+str(c_ex_loss)+' cont ex accu biased: '+str(c_ex_accu_biased)+' cont external val accuracy: '+str(c_ex_accuracy))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_test",
   "language": "python",
   "name": "torch_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
