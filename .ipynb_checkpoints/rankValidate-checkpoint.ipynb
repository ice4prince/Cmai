{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b20977d3-cd1b-4b01-aedc-439817430e69",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2023-07-05\n",
      "2023-07-05 16:36:56.929234\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SubsetRandomSampler,Sampler\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import colorama\n",
    "from colorama import Fore,Back,Style\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Today's date:\",date.today())\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "546dd6e3-1c8b-4dbc-a9ec-68c752463865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Parameters for pair model.')\n",
    "\n",
    "# # Add a optional argument\n",
    "# parser.add_argument('--code', type=str, help='the CLAnO directory',default = '/project/DPDS/Wang_lab/shared/BCR_antigen/code/CLAnO')\n",
    "# parser.add_argument('--bcr',type = str, help = 'the input files for BCRs',default = '/project/DPDS/Wang_lab/shared/BCR_antigen/code/CLAnO/data/example/Jun_bcr.csv')\n",
    "# parser.add_argument('--antigen',type = str, help = 'the fasta file of antigens to input',default = '/project/DPDS/Wang_lab/shared/BCR_antigen/code/CLAnO/data/example/Jun_bcr.csv')\n",
    "# # parser.add_argument('--mode', type=str, default='binary', choices=['binary', 'continuous'], help='Your choice: binary or continuous. Default is binary.')\n",
    "# parser.add_argument('--continuous', action='store_true', help='swtich the mode from binary to continuous, default mode is binary.')\n",
    "# parser.add_argument('--species', action='store_true', help='match the species of background BCR to the target BCR. NOTE: the species MUST BE specified and unique in the target BCR input.')\n",
    "# # parser.add_argument('--cut_off', type=int, help='the maximum of length of sequencing, over which will be deleted. default is 1800',default = 1800)\n",
    "# parser.add_argument('--seed', type=int, help='the seed for the first 100 background BCRs. To use the prepared embeded 100 BCRs, keep the seed to default 1',default = 1)\n",
    "# parser.add_argument('--Limit', type = float, help = 'the size limit for the antigens in gpu.',default = 5)\n",
    "# parser.add_argument('--max_num', type = float, help = 'the size limit for the antigens in gpu.',default = 10)\n",
    "# parser.add_argument('--group_size', type = int, help = 'the leap to change the antigen unless there are not enough entries.',default = 50)\n",
    "# parser.add_argument('--verbose', action='store_true', help='Enable verbose output, default is False.')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# CLANO_DIR = args.code\n",
    "# BCR_INPUT = args.bcr\n",
    "# ANTIGEN_INPUT = args.antigen\n",
    "# CUTOFF = args.cut_off\n",
    "# SEED = args.seed\n",
    "# LIMIT = args.Limit\n",
    "# Max_num = args.max_num\n",
    "# GROUP_SIZE = args.group_size\n",
    "# VERBOSE = args.verbose\n",
    "# CONT = args.continuous\n",
    "# MATCHING_SPECIES = args.species\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "83b8b4c7-a855-4eae-b424-88062cd243e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLANO_DIR = '/project/DPDS/Wang_lab/shared/BCR_antigen/code/CLAnO'\n",
    "ANTIGEN_INPUT = \"/project/DPDS/Wang_lab/shared/BCR_antigen/code/CLAnO/data/example/cleaned.antigen.Jun.fasta\"\n",
    "BCR_INPUT = \"/project/DPDS/Wang_lab/shared/BCR_antigen/code/CLAnO/data/example/Jun_bcr.csv\"\n",
    "#CUTOFF = 1800\n",
    "SEED = 1\n",
    "GROUP_SIZE = 50\n",
    "LIMIT = 5\n",
    "Max_num = 10\n",
    "VERBOSE = False\n",
    "CONT = False\n",
    "MATCHING_SPECIES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "75fec86a-20a5-4c08-ba05-f4bc03931578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.chdir(CLANO_DIR)\n",
    "BACKGROUND = 'data/background/backgroundBCR.csv'\n",
    "NPY_DIR = 'data/intermediates/NPY' ###need to add a command to move the pair.npy under results/pred/ to the intermediates/\n",
    "MODEL = 'models/binary_model.pth'\n",
    "\n",
    "from wrapV.Vwrap import embedV ##input needs to be list of strings\n",
    "from wrapCDR3.CDR3wrap import embedCDR3 ##input needs to be list of strings\n",
    "\n",
    "CHANNEL_ANTIGEN = 600\n",
    "CLIP = None\n",
    "LR = 0.005\n",
    "T = 0.005\n",
    "LAMBDA = 0\n",
    "w = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "db140389-d4f2-4616-a43e-195d9a8766f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n",
      "[GCC 10.3.0]\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('system version:',sys.version)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:',device)\n",
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6ff55a61-fe1a-488b-b320-9d155365c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering binary mode now...\n"
     ]
    }
   ],
   "source": [
    "if CONT:\n",
    "    MODE = 'continuous'\n",
    "    print('mode switching ...')\n",
    "else:\n",
    "    MODE = 'binary'\n",
    "print('Entering',MODE,'mode now...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "11715ba4-faf2-4488-be9b-f7bbc296609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bcr_file = pd.read_csv(BCR_INPUT) # required columns 'Vh','CDR3h', optional 'species'\n",
    "background = pd.read_csv(BACKGROUND) # with columns 'Vh','CDR3h','file','species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "306a450a-14df-426e-a6ec-aa515082ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target species are not unique or not in the background species.\n"
     ]
    }
   ],
   "source": [
    "if 'BCR_species' in target_bcr_file.columns:\n",
    "    unique_values = target_bcr_file['BCR_species'].dropna().unique()\n",
    "    if len(unique_values) == 1 and unique_values[0] in {'human', 'mouse'}:\n",
    "        species = unique_values[0]\n",
    "        print('The species is:',species)\n",
    "        if MATCHING_SPECIES:\n",
    "            print('matching the background species...')\n",
    "            backgroud_bcr_file = backgroud_bcr_file[backgroud_bcr_file['species']==species]\n",
    "    else:\n",
    "        print('The target species are not unique or not in the background species.')\n",
    "else:\n",
    "    print('No species are specified!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "735a779b-46c6-4bab-8624-0c7d85016b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def get_now():\n",
    "#     now = str(datetime.now())\n",
    "#     tosec = now.split('.')[0]\n",
    "#     out_now = tosec.split(' ')[0]+'_'+tosec.split(' ')[1].replace(':',\"-\")\n",
    "#     return out_now\n",
    "\n",
    "# def filter_big_antigens(dataset,cutoff):\n",
    "#     dataset['aalens']=list(map(len,dataset['Antigen'].tolist()))\n",
    "#     data_filtered = dataset.loc[dataset['aalens']< cutoff]\n",
    "#     print('After removing antigens larger than '+str(cutoff)+', '+str(100*data_filtered.shape[0]/dataset.shape[0])+'% antigens remained.')\n",
    "#     return data_filtered\n",
    "\n",
    "def check_bad_bcr(seq):\n",
    "    allowed_letters = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    uppercase_string = seq.upper()\n",
    "    return not any(char not in allowed_letters for char in uppercase_string)\n",
    "\n",
    "def filter_bad_bcr(df):\n",
    "    substrings = ['Vh', 'CDR3h', 'Antigen']\n",
    "    some_columns = [col for col in df.columns if any(sub in col for sub in substrings)]\n",
    "    for col in some_columns:\n",
    "        df[col] = df[col].apply(lambda x: x.replace(' ', ''))\n",
    "    mask = df[some_columns].applymap(check_bad_bcr)\n",
    "    filtered_df = df[mask.all(axis=1)]\n",
    "    return filtered_df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = filter_bad_bcr(df)\n",
    "#     if 'Antigen' in df.columns:\n",
    "#         df = filter_big_antigens(df,cutoff)\n",
    "#         df = df.assign(antigen_index = df['Project'] + '/' + df['id'].astype(str))\n",
    "#     df = df.sort_values('antigen_index',)\n",
    "    df = df.assign(record_id = ['record_' + str(s) for s in range(df.shape[0])])\n",
    "    return df\n",
    "\n",
    "# def has_space(string):\n",
    "#     return \" \" in string\n",
    "# def check_bad_letters(df,binary = True):\n",
    "#     substrings = ['Vh', 'CDR3h', 'Antigen']\n",
    "#     some_columns = [col for col in df_copy.columns if any(sub in col for sub in substrings)]\n",
    "#     for col in some_columns:\n",
    "#         if not all(df[col].apply(check_bad_bcr)):\n",
    "#             print(Fore.RED +str(col)+' contains uncommon aa or symbols!')\n",
    "#         else:\n",
    "#             print(Fore.GREEN +str(col)+' PASS!')\n",
    "#     print(Style.RESET_ALL)\n",
    "\n",
    "def build_BCR_dict(dataset,colname,precise = False):\n",
    "    cols = dataset.filter(like = colname)\n",
    "    uniq_keys = pd.unique(cols.values.ravel()).tolist()\n",
    "    if colname == 'CDR3h':\n",
    "        uniq_embedding,_,input_keys = embedCDR3(uniq_keys,precise = precise)\n",
    "    elif colname == 'Vh':\n",
    "        uniq_embedding,_,input_keys = embedV(uniq_keys,precise = precise)\n",
    "    i = 0\n",
    "    mydict = {}\n",
    "    for key in input_keys:\n",
    "        mydict[key] = uniq_embedding[i]\n",
    "        i += 1\n",
    "    return(mydict)\n",
    "\n",
    "# def predict_size(len,datatype = 'float32'):\n",
    "#     # Define the shape of the tensor\n",
    "#     shape = [1, len, len, 318]\n",
    "#     element_size = np.dtype(datatype).itemsize\n",
    "#     num_elements = math.prod(shape)\n",
    "#     tensor_size = num_elements * element_size\n",
    "# #    size_gb = tensor_size/(1024**3)\n",
    "# #    print(f\"The tensor takes up {size_gb:.2f} GB of memory.\")\n",
    "# #    size_gb = tensor_size/(1024*1024*1024)\n",
    "# #    print(f\"Tensor size: {size_gb:.2f} GB\")\n",
    "#     return tensor_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "222fd015-4295-4880-be46-ec78e03ca5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backgroud = preprocess(backgroud_bcr_file,CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ca511538-d674-4cb4-bea3-1fd47b9ab9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backgroud.to_csv(BACKGROUND,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d7fc5709-9f4a-4d69-89be-f70e703e68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 background BCR.\n",
    "# Sample 1% of the rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c29b9bae-a3de-4e5a-ab48-8de9e3b51bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SEED == 1:\n",
    "    with open('data/background/default100_V_dict.pkl','rb') as f:\n",
    "        Vh_dict = pickle.load(f)\n",
    "    with open('data/background//default100_CDR3_dict.pkl','rb') as f:\n",
    "        CDR3h_dict = pickle.load(f)\n",
    "else:\n",
    "    back100 = background.sample(frac=1/10000, random_state=SEED)\n",
    "    Vh_dict = build_BCR_dict(subsample,'Vh',precise = True)\n",
    "    CDR3h_dict = build_BCR_dict(subsample,'CDR3h',precise = True)\n",
    "# with open('data/background/default100_V_dict.pkl','wb') as f:\n",
    "#     pickle.dump(Vh_dict, f)\n",
    "# with open('data/background//default100_CDR3_dict.pkl','wb') as f:\n",
    "#     pickle.dump(CDR3h_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9142c82d-86c9-4585-b4fb-ac8b261e61f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['VCL_isoform2', 'VCL_isoform1', 'VCL_isoform3', 'CH25H', 'LZTS3_isoform1', 'LZTS3_isoform2', 'RSPO2_isoform1', 'RSPO2_isoform2', 'RSPO2_isoform3'])\n"
     ]
    }
   ],
   "source": [
    "antigen_dict = {}\n",
    "with open(ANTIGEN_INPUT, \"r\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        # Access the ID and sequence of each record\n",
    "        antigen_dict[record.id] = str(record.seq)\n",
    "print(antigen_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f68a8-83b6-4999-9796-5b9108b6b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "##MARK HERE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e4f21e4f-43cb-4be6-ae90-6f3690abba2b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "#from torch.utils.data import Dataset\n",
    "#import numpy as np\n",
    "class rankDataset(Dataset):\n",
    "    def __init__(self, antigen, dataframe, antigen_dict, cdr3_dict, v_dict, antigen_fpath_dict,subsample_ratio=1/10000,seed=SEED):\n",
    "        self.seed = seed\n",
    "        self.antigen_fpath_dict = antigen_fpath_dict\n",
    "        self.antigen_dict = antigen_dict\n",
    "        self.cdr3_dict = cdr3_dict\n",
    "        self.v_dict = v_dict\n",
    "        self.background = self.subsample_data(dataframe, subsample_ratio=subsample_ratio)\n",
    "        self.bcr_pool = self.background[['Vh','CDR3h']].to_dict(orient='records')\n",
    "#        print(len(self.bcr_pool))\n",
    "        self.antigen_feat = self.extract_antigen(antigen)[0].to(device)\n",
    "        self.lengthen = len(self.antigen_dict[antigen])\n",
    "#        print(self.lengthen)\n",
    "\n",
    "    def subsample_data(self, dataframe, subsample_ratio):\n",
    "        if subsample_ratio < 1.0:\n",
    "            if not subsample_ratio == 1/10000:\n",
    "                self.seed = None\n",
    "            return dataframe.sample(frac=subsample_ratio,random_state=self.seed)\n",
    "        else:\n",
    "            return dataframe    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bcr_dict = self.bcr_pool[idx]\n",
    "#        index_key = bcr_dict['ID']\n",
    "        v_key = bcr_dict['Vh']\n",
    "        cdr3_key = bcr_dict['CDR3h']        \n",
    "        bcr_feat = self.__embedding_BCR(cdr3_key,v_key,precise = True)\n",
    "        pair_feat = self.__comb_embed_gpu(bcr_feat)\n",
    "        return pair_feat\n",
    "\n",
    "    def __comb_embed_gpu(self,bcr_feat):\n",
    "        if MODE == 'binary':\n",
    "            lengthen = CHANNEL_ANTIGEN\n",
    "        else:\n",
    "            lengthen = self.lengthen\n",
    "        single_antigen_g = self.antigen_feat  \n",
    "        single_BCR_g = torch.from_numpy(bcr_feat).to(device)\n",
    "        BCR_t = torch.tile(single_BCR_g,(lengthen,lengthen,1))\n",
    "        pair_feat_g = torch.cat((single_antigen_g,BCR_t),dim=2)\n",
    "        del single_BCR_g,BCR_t\n",
    "        torch.cuda.empty_cache()\n",
    "        return pair_feat_g\n",
    "\n",
    "    def extract_antigen(self,antigen_name):\n",
    "        try:\n",
    "            antigen_import = np.load(str(self.antigen_fpath_dict+'/'+antigen_name+'.pair.npy'))/w\n",
    "#             if not antigen_import.shape[1] == self.lengthen:\n",
    "#                 print(Fore.RED + 'antigen ' +str(antigen_name)+' embedding'+str(antigen_import.shape[1])+' is NOT in the correct shape '+str(self.lens_dict[antigen_name])+'!'+ Style.RESET_ALL)\n",
    "#                 exit()            \n",
    "            single_antigen = torch.from_numpy(antigen_import).float()\n",
    "            if MODE == 'binary':\n",
    "                single_antigen = self.pool_antigen(single_antigen,CHANNEL_ANTIGEN) ###ON CPU\n",
    "        except ValueError:\n",
    "            print('The embedding of antigen %s cannot be found!' % antigen_name)        \n",
    "        return single_antigen\n",
    "\n",
    "    def __embedding_BCR(self,cdr3_seq,v_seq,precise = True):\n",
    "        if cdr3_seq not in self.cdr3_dict:\n",
    "            cdr3_feat,*_ = embedCDR3([cdr3_seq],precise = precise)\n",
    "            cdr3_feat = cdr3_feat[0]\n",
    "            self.cdr3_dict[cdr3_seq]=cdr3_feat\n",
    "        else:\n",
    "            cdr3_feat = self.cdr3_dict[cdr3_seq]\n",
    "        if v_seq not in self.v_dict:\n",
    "            v_feat,*_ = embedV([v_seq],precise = precise)\n",
    "            v_feat = v_feat[0]\n",
    "            self.v_dict[v_seq]=v_feat\n",
    "        else:\n",
    "            v_feat = self.v_dict[v_seq]\n",
    "        bcr_feat = np.concatenate((cdr3_feat,v_feat))\n",
    "        return bcr_feat\n",
    "    \n",
    "    def pool_antigen(self,antigen_tensor,out_n_channel):\n",
    "#        lengthen = antigen_input.shape[1]\n",
    "        pooling_layer = nn.AdaptiveAvgPool2d((out_n_channel,out_n_channel))\n",
    "        output = pooling_layer(antigen_tensor.permute(0,3,1,2)).permute(0,2,3,1)\n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.bcr_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1f67f1c0-74ae-4b66-9969-76832eebfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class checkDataset(Dataset):\n",
    "    def __init__(self, antigen, dataframe, antigen_dict, antigen_fpath_dict):\n",
    "        self.antigen_fpath_dict = antigen_fpath_dict\n",
    "        self.antigen_dict = antigen_dict\n",
    "        self.data = dataframe\n",
    "        self.bcr_pool = self.data[['ID','Vh','CDR3h']].to_dict(orient='records')\n",
    "        self.antigen_feat = self.extract_antigen(antigen)[0].to(device)\n",
    "        self.lengthen = len(self.antigen_dict[antigen])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bcr_dict = self.bcr_pool[idx]\n",
    "        index_key = bcr_dict['ID']\n",
    "        v_key = bcr_dict['Vh']\n",
    "        cdr3_key = bcr_dict['CDR3h']        \n",
    "        bcr_feat = self.__embedding_BCR(cdr3_key,v_key,precise = True)\n",
    "        pair_feat = self.__comb_embed_gpu(bcr_feat)\n",
    "        return pair_feat,index_key\n",
    "\n",
    "    def __comb_embed_gpu(self,bcr_feat):\n",
    "        if MODE == 'binary':\n",
    "            lengthen = CHANNEL_ANTIGEN\n",
    "        else:\n",
    "            lengthen = self.lengthen\n",
    "        single_antigen_g = self.antigen_feat  \n",
    "        single_BCR_g = torch.from_numpy(bcr_feat).to(device)\n",
    "        BCR_t = torch.tile(single_BCR_g,(lengthen,lengthen,1))\n",
    "        pair_feat_g = torch.cat((single_antigen_g,BCR_t),dim=2)\n",
    "        del single_BCR_g,BCR_t\n",
    "        torch.cuda.empty_cache()\n",
    "        return pair_feat_g\n",
    "\n",
    "    def extract_antigen(self,antigen_name):\n",
    "        try:\n",
    "            antigen_import = np.load(str(self.antigen_fpath_dict+'/'+antigen_name+'.pair.npy'))/w\n",
    "#             if not antigen_import.shape[1] == self.lengthen:\n",
    "#                 print(Fore.RED + 'antigen ' +str(antigen_name)+' embedding'+str(antigen_import.shape[1])+' is NOT in the correct shape '+str(self.lens_dict[antigen_name])+'!'+ Style.RESET_ALL)\n",
    "#                 exit()            \n",
    "            single_antigen = torch.from_numpy(antigen_import).float()\n",
    "            if MODE == 'binary':\n",
    "                single_antigen = self.pool_antigen(single_antigen,CHANNEL_ANTIGEN) ###ON CPU\n",
    "        except ValueError:\n",
    "            print('The embedding of antigen %s cannot be found!' % antigen_name)        \n",
    "        return single_antigen\n",
    "\n",
    "    def __embedding_BCR(self,cdr3_seq,v_seq,precise = True):\n",
    "#         if cdr3_seq not in self.cdr3_dict:\n",
    "        cdr3_feat,*_ = embedCDR3([cdr3_seq],precise = precise)\n",
    "        cdr3_feat = cdr3_feat[0]\n",
    "#             self.cdr3_dict[cdr3_seq]=cdr3_feat\n",
    "#         else:\n",
    "#             cdr3_feat = self.cdr3_dict[cdr3_seq]\n",
    "#         if v_seq not in self.v_dict:\n",
    "        v_feat,*_ = embedV([v_seq],precise = precise)\n",
    "        v_feat = v_feat[0]\n",
    "#             self.v_dict[v_seq]=v_feat\n",
    "#         else:\n",
    "#             v_feat = self.v_dict[v_seq]\n",
    "        bcr_feat = np.concatenate((cdr3_feat,v_feat))\n",
    "        return bcr_feat\n",
    "    \n",
    "    def pool_antigen(self,antigen_tensor,out_n_channel):\n",
    "#        lengthen = antigen_input.shape[1]\n",
    "        pooling_layer = nn.AdaptiveAvgPool2d((out_n_channel,out_n_channel))\n",
    "        output = pooling_layer(antigen_tensor.permute(0,3,1,2)).permute(0,2,3,1)\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bcr_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "419ebe0d-ff18-4aac-8a6d-52705d335601",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class SelfAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of SelfAttentionPooling\n",
    "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
    "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim,hidden_dim):\n",
    "        super(SelfAttentionPooling, self).__init__()\n",
    "#        hidden_dim=10\n",
    "        self.W1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.W2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "\n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "\n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "        att_w = F.softmax(self.W2(self.relu(self.W1(batch_rep))).squeeze(-1),dim=1).unsqueeze(-1)\n",
    "\n",
    "        utter_rep = torch.sum(batch_rep* att_w, dim=1)\n",
    "\n",
    "        return utter_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "863dfe4f-8ea5-46be-b50e-4103fc4bdd57",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class mix_model(nn.Module):\n",
    "    def __init__(self,mode='binary'):\n",
    "        super(mix_model,self).__init__()\n",
    "        if mode =='binary':\n",
    "            self.model1 = nn.Sequential(\n",
    "                nn.Linear(318,40),#.to(torch.float64),\n",
    "                # in (1,len,len,318)\n",
    "                # out (1,len,len.50)\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Linear(40,30),#.to(torch.float64),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Linear(30,20),#.to(torch.float64),\n",
    "                # out (1,len,len,20)\n",
    "                nn.LeakyReLU(0.1)\n",
    "            )\n",
    "        else:\n",
    "            self.model10 = nn.Sequential(\n",
    "                nn.Linear(318,40),#.to(torch.float64),\n",
    "                # in (1,len,len,318)\n",
    "                # out (1,len,len.50)\n",
    "                nn.LeakyReLU(0.1),\n",
    "            )\n",
    "\n",
    "            self.model11 = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d((CHANNEL_ANTIGEN,CHANNEL_ANTIGEN))\n",
    "            )\n",
    "\n",
    "            self.model12 = nn.Sequential(\n",
    "                nn.Linear(40,30),#.to(torch.float64),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Linear(30,20),#.to(torch.float64),\n",
    "                # out (1,len,len,20)\n",
    "                nn.LeakyReLU(0.1)\n",
    "            )\n",
    "        self.model2 = SelfAttentionPooling(input_dim=20,hidden_dim=30)\n",
    "        self.model2_1 = SelfAttentionPooling(input_dim=20,hidden_dim=30)\n",
    "        # input_dim = hidden size (number of channels)\n",
    "#        self.model2 = nn.MultiheadAttention(embed_dim=20,num_heads=N_HEADS,batch_first=True)\n",
    "        self.model3 = nn.Sequential(\n",
    "            nn.Linear(20,15),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(15,1)\n",
    "        )\n",
    "#         self.alpha1 = nn.Parameter(torch.randn(1))\n",
    "#         self.beta1 = nn.Parameter(torch.randn(1))\n",
    "#         self.alpha2 = nn.Parameter(torch.randn(1))\n",
    "#         self.beta2 = nn.Parameter(torch.randn(1))\n",
    "    def forward(self,x,mode='binary'): ###because in getitem, return is .cuda(), now input is on gpu\n",
    "#         x = torch.empty(0)\n",
    "#         x = x.to(device)\n",
    "#        x = x.permute(0,2,1,3)\n",
    "#         print('after permute',x.shape)\n",
    "        x0 = torch.empty(0)\n",
    "        x0 = x0.to(device)\n",
    "        if mode=='binary':\n",
    "            x = self.model1(x)\n",
    "            for i in range(len(x)):\n",
    "                k = x[i]\n",
    "                k = self.model2(k).unsqueeze(0)\n",
    "                k = self.model2_1(k)\n",
    "                x0 = torch.cat((x0, k), dim=0)\n",
    "        else:\n",
    "            x = self.model10(x)\n",
    "            for i in range(len(x)):\n",
    "                k = x[i]\n",
    "                k=k.permute(2,0,1)\n",
    "                k=self.model11(k)\n",
    "                k=k.permute(1,2,0)\n",
    "                k=self.model12(k)\n",
    "                k = self.model2(k).unsqueeze(0)\n",
    "                k = self.model2_1(k)\n",
    "                x0 = torch.cat((x0, k), dim=0)\n",
    "        x0 = F.normalize(x0)\n",
    "        x0 = self.model3(x0).squeeze()\n",
    "#        if binary:\n",
    "        out  = x0\n",
    "        return(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2e8930ca-00cc-4a30-b96e-73b082ee8182",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def background_scores(dataloader,model):\n",
    "    score_ls = []\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        score = model(batch)\n",
    "        score_ls.append(score.item())\n",
    "    return score_ls\n",
    "\n",
    "def check_scores(dataloader,model):\n",
    "    score_dict = {}\n",
    "    model.eval()\n",
    "    for batch, index in dataloader:\n",
    "        score_dict[index] = model(batch).item()\n",
    "    return score_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a9e679b4-f75a-44ae-8c6d-7c872027e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_rank(number, my_list):\n",
    "    # Step 1: Sort the list in ascending order\n",
    "    sorted_list = sorted(my_list)\n",
    "\n",
    "    # Step 2: Insert the number into the sorted list while maintaining the sorted order\n",
    "    from bisect import insort_left\n",
    "    insort_left(sorted_list, number)\n",
    "\n",
    "    # Step 3: Find the index of the inserted number in the list\n",
    "    index = sorted_list.index(number)\n",
    "\n",
    "    # Step 4: Calculate the percentage rank using the index and the length of the list\n",
    "    percentage_rank = (index / len(sorted_list))\n",
    "\n",
    "    return percentage_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "77810a42-e9f9-42ac-baf8-49f6e810dc31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model_mix = mix_model(mode=MODE)\n",
    "checkpoint = torch.load(MODEL)\n",
    "model_mix.load_state_dict(checkpoint)\n",
    "model_mix.to(device)\n",
    "optimizer = torch.optim.Adam(model_mix.parameters(),lr=LR)\n",
    "scheduler = ReduceLROnPlateau(optimizer,mode = 'min',factor=0.1,patience =10,verbose=False,threshold_mode='rel',threshold=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2e8dc86f-a70d-439f-b7cc-2a9349a4531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARK HERE: Is it Possible to remove the alpha1 beta1 alpha2 beta2 from checkpoint?\n",
    "# REMEMBER to write if 1%, expand the subsample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "00f53d55-9c72-4ea1-ad6f-38bd1e29fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove the parameters you don't want\n",
    "# del checkpoint['alpha1']\n",
    "# del checkpoint['alpha2']\n",
    "# del checkpoint['beta1']\n",
    "# del checkpoint['beta2']\n",
    "\n",
    "# # Save the modified checkpoint\n",
    "# torch.save(checkpoint,MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0e78a365-5d77-4d62-835a-6e8f9e67b128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For antigen: VCL_isoform2\n",
      "the binding rank of the tested bcr is: {'HS103': 0.594059405940594, 'HS503': 0.3069306930693069}\n",
      "For antigen: VCL_isoform1\n",
      "the binding rank of the tested bcr is: {'HS103': 0.5841584158415841, 'HS503': 0.19801980198019803}\n",
      "For antigen: VCL_isoform3\n",
      "the binding rank of the tested bcr is: {'HS103': 0.7821782178217822, 'HS503': 0.6138613861386139}\n",
      "For antigen: CH25H\n",
      "the binding rank of the tested bcr is: {'HS103': 0.7029702970297029, 'HS503': 0.5742574257425742}\n",
      "For antigen: LZTS3_isoform1\n",
      "the binding rank of the tested bcr is: {'HS103': 0.297029702970297, 'HS503': 0.0}\n",
      "For antigen: LZTS3_isoform2\n",
      "the binding rank of the tested bcr is: {'HS103': 0.21782178217821782, 'HS503': 0.0}\n",
      "For antigen: RSPO2_isoform1\n",
      "the binding rank of the tested bcr is: {'HS103': 0.7524752475247525, 'HS503': 0.594059405940594}\n",
      "For antigen: RSPO2_isoform2\n",
      "the binding rank of the tested bcr is: {'HS103': 0.6831683168316832, 'HS503': 0.49504950495049505}\n",
      "For antigen: RSPO2_isoform3\n",
      "the binding rank of the tested bcr is: {'HS103': 0.46534653465346537, 'HS503': 0.0594059405940594}\n"
     ]
    }
   ],
   "source": [
    "for antigen in antigen_dict.keys():\n",
    "    print('For antigen:',antigen)\n",
    "#     background_loader =rankDataset(antigen, background, antigen_dict, CDR3h_dict, Vh_dict,NPY_DIR,subsample_ratio=1/10000,seed=1)\n",
    "    background_dataset =rankDataset(antigen, background, antigen_dict, CDR3h_dict, Vh_dict,NPY_DIR,subsample_ratio=1/10000,seed=1)\n",
    "    background_loader = DataLoader(background_dataset, 1)\n",
    "    back_scores = background_scores(background_loader,model_mix)\n",
    "    target_dataset =checkDataset(antigen, target_bcr_file, antigen_dict, NPY_DIR)\n",
    "    target_loader =DataLoader(target_dataset, 1)\n",
    "    target_score = check_scores(target_loader,model_mix)\n",
    "    #print('the binding score of the tested bcr is:',target_score)\n",
    "    rank = {}\n",
    "    for bcr, score in target_score.items():\n",
    "        rank[bcr[0]]= locate_rank(score,back_scores)\n",
    "    print('the binding rank of the tested bcr is:',rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce965211-f8b5-4e57-b371-ba15f6fb956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mark here to export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_new",
   "language": "python",
   "name": "pytorch_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
