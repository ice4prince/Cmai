{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286e8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd902cd6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2023-06-30\n",
      "2023-06-30 14:20:36.233208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SubsetRandomSampler,Sampler\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import colorama\n",
    "from colorama import Fore,Back,Style\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Today's date:\",date.today())\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6a0b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Parameters for pair model.')\n",
    "\n",
    "# # Add a optional argument\n",
    "# parser.add_argument('--input', type=str, help='the input file tag',default = '3diff_easy_negative')\n",
    "# parser.add_argument('--inputE',type = str, help = 'the input file name excluding csv for external validation',default = 'exVal_r2')\n",
    "# parser.add_argument('--inputCE',type = str, help = 'the input file name excluding csv for continuous external validation',default = 'ex_cont_r4')\n",
    "# parser.add_argument('--batch_size', type=int, help='the batch size, default is 1',default = 1)\n",
    "# parser.add_argument('--seed', type=int, help='the seed for dataloader, default is None',default = None)\n",
    "# parser.add_argument('--epoch', type=int, help='the maximum of epoch, default is 50',default = 50)\n",
    "# parser.add_argument('--tag', type=int, help='the tag to specify the running',default = 1)\n",
    "# parser.add_argument('--initial_learning_rate', type=float, help='the starting leanring rate, default is 0.05', default=0.01)\n",
    "# parser.add_argument('--delta', type=float, help='the minimum difference between better and worse, default is 0.01', default=0.01)\n",
    "# parser.add_argument('--Lambda', type=float, help='the parameter adjustable for loss to avoid INF and NAN', default=0)\n",
    "# parser.add_argument('--weight', type=float, help='the parameter adjustable to shrink embedding of antigens to input', default=100)\n",
    "# parser.add_argument('--cut_off', type=int, help='the maximum of length of sequencing, over which will be deleted. default is 1800',default = 1800)\n",
    "# parser.add_argument('--group_size', type = int, help = 'the leap to change the antigen unless there are not enough entries.',default = 100)\n",
    "# parser.add_argument('--Limit', type = float, help = 'the size limit for the antigens in gpu.',default = 5)\n",
    "# parser.add_argument('--max_num', type = float, help = 'the size limit for the antigens in gpu.',default = 10)\n",
    "# parser.add_argument('--model',type = str, help = 'the model file to import',default = None)\n",
    "# parser.add_argument('--resize',type = int, help = 'the length after resizing',default = 500)\n",
    "# parser.add_argument('--clip',type = int, help = 'the clip value',default = 1)\n",
    "# parser.add_argument('--each',type = int, help = 'the number of pairs of each strata.',default = 50)\n",
    "# parser.add_argument('--small_sample',type = int, help = 'whether downsample for experiment.',default = None)\n",
    "# parser.add_argument('--inter_sub',type = float, help = 'the ratio of subsampling for training and internal validation.',default = 0.01)\n",
    "# parser.add_argument('--outer_sub',type = float, help = 'the ratio of subsampling for the external validation',default = 1)\n",
    "# # parser.add_argument('--model',type = str, help = 'the model file to import',default = '/project/DPDS/Wang_lab/shared/BCR_antigen/data/output/2023-02-18_12-33-25_tag2/model_comb/Batch50BatchNumber_600Lr0.01Epoch54tag2_easy_neg.pth')\n",
    "# parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n",
    "\n",
    "# # Add a switch (flag) argument\n",
    "# #parser.add_argument('--verbose', action='store_true', help='Print verbose output')\n",
    "# # Parse the command-line arguments\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# INPUT = args.input\n",
    "# INPUT_E = args.inputCE\n",
    "# INPUT_CE = args.inputCE\n",
    "# #NUMBER_BATCH = args.number_of_batch\n",
    "# LR = args.initial_learning_rate\n",
    "# EPOCH = args.epoch\n",
    "# CUTOFF = args.cut_off\n",
    "# TAG = args.tag\n",
    "# SEED = args.seed\n",
    "# T = args.delta\n",
    "# LAMBDA = args.Lambda\n",
    "# w = args.weight\n",
    "# NPY_DIR ='/project/DPDS/Wang_lab/shared/BCR_antigen/data/rfscript/Antigen_embed/CLEANED'\n",
    "# LIMIT = args.Limit\n",
    "# #NPY_DIR ='/project/DPDS/Wang_lab/shared/BCR_antigen/data/rfscript/Antigen_embed/CLEANED'\n",
    "# VERBOSE = args.verbose\n",
    "# GROUP_SIZE = args.group_size\n",
    "# MODEL = args.model\n",
    "# Max_num = args.max_num\n",
    "# Small_sample = args.small_sample\n",
    "# EX_SUBSAMPLE = args.outer_sub\n",
    "# IN_SUBSAMPLE = args.inter_sub\n",
    "# CHANNEL_ANTIGEN = args.resize\n",
    "# CLIP = args.clip\n",
    "# EACH = args.each\n",
    "#\n",
    "\n",
    "INPUT = '3diff_easy_negative_3rep'\n",
    "# OUT_DIR = '2023-05-17_09-46-34_tag11_mark0.946Epoch20'\n",
    "BATCH_SIZE = 1\n",
    "LR = 0.005\n",
    "EPOCH = 50\n",
    "CUTOFF = 1800\n",
    "TAG = 1\n",
    "SEED = 44\n",
    "T = 0.005\n",
    "LAMBDA = 0\n",
    "w = 100\n",
    "LIMIT = 5\n",
    "NPY_DIR ='/project/DPDS/Wang_lab/shared/BCR_antigen/data/rfscript/Antigen_embed/CLEANED'\n",
    "VERBOSE = False\n",
    "GROUP_SIZE = 50\n",
    "Max_num = 2\n",
    "Small_sample = None\n",
    "EX_SUBSAMPLE1 = 0.99\n",
    "EX_SUBSAMPLE2 = 0\n",
    "IN_SUBSAMPLE1 = 0.3\n",
    "IN_SUBSAMPLE2 = 0.01\n",
    "CHANNEL_ANTIGEN = 600\n",
    "MODEL = ''\n",
    "CLIP = None\n",
    "#INPUT = '3diff_easy_negative_5rep'\n",
    "INPUT_E = 'exVal_r5'\n",
    "INPUT_CE = 'ex_cont_r5'\n",
    "EACH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62174fa7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('batch size:',BATCH_SIZE,\n",
    "      '\\ninput label:',INPUT,\n",
    "      '\\ninput external:',INPUT_E,\n",
    "      '\\ninput external continuous:',INPUT_CE,\n",
    "#       '\\noutput dir:',OUT_DIR,\n",
    "      '\\nEpoch max:',EPOCH,\n",
    "      '\\nLearning rate:',LR,\n",
    "      '\\nCut off:',CUTOFF,\n",
    "      '\\ntag:',TAG,\n",
    "      '\\ndelta:',T,\n",
    "      '\\nLambda:',LAMBDA,\n",
    "      '\\nweight:',w,\n",
    "      '\\nlimit:',LIMIT,\n",
    "      '\\nverbose:',VERBOSE,\n",
    "      '\\ngroup_size:',GROUP_SIZE,\n",
    "      '\\nmodel:',MODEL,\n",
    "      '\\nmax number of antigen:',Max_num,\n",
    "      '\\number of channels:',CHANNEL_ANTIGEN,\n",
    "      '\\nexternal subsample:',EX_SUBSAMPLE1,\n",
    "      '\\ninternal subsample:',IN_SUBSAMPLE1,\n",
    "      '\\nexternal subsample:',EX_SUBSAMPLE2,\n",
    "      '\\ninternal subsample:',IN_SUBSAMPLE2,\n",
    "      '\\nclip:',CLIP,\n",
    "      '\\neach',EACH,\n",
    "     '\\nsmall_sample:',Small_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba9240",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "os.chdir('/project/DPDS/Wang_lab/shared/BCR_antigen/code/DeLAnO/')\n",
    "#mp.set_sharing_strategy('file_system')\n",
    "#mp.set_start_method('spawn')\n",
    "from wrapV.Vwrap import embedV\n",
    "from wrapCDR3.CDR3wrap import embedCDR3\n",
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61134e31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import gc\n",
    "# del model_mix\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2b62b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##Inputing original datasets\n",
    "input_ori = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/'+INPUT+'.csv',index_col = 0)\n",
    "total_N_row = input_ori.shape[0]\n",
    "\n",
    "#exVal_ori = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/results_hard_negative/results_hard_negative.csv')\n",
    "exVal_ori = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/'+INPUT_E+'.csv',index_col=0)\n",
    "total_N_ex = exVal_ori.shape[0]\n",
    "\n",
    "cont_val_ori = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/'+INPUT_CE+'.csv',index_col=0)\n",
    "# cont_val.head()\n",
    "# cont_train_10x = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/train_validate/continuous_training/cont_training_compare_10x_less10.csv')\n",
    "# cont_train_libra = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/train_validate/continuous_training/cont_training_compare_libra_less10.csv')\n",
    "cont_train_ori = pd.read_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/train_validate/continuous_training/cont_train_cutoff_dist6.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe2596",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cont_train_ori=cont_train_ori[cont_train_ori['distance']<4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ef3af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(cont_train_ori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbcbb09",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# exVal_ori.loc[exVal_ori['id']=='EbolaGP','Antigen']='mgvtgilqlprdrfkrtsfflwviilfqrtfsiplgvihnstlqvsdvdklvcrdklsstnqlrsvglnlegngvatdvpsatkrwgfrsgvppkvvnyeagewaencynleikkpdgseclpaapdgirgfprcryvhkvsgtgpcagdfafhkegafflydrlastviyrgttfaegvvaflilpqakkdffsshplrepvnatedpssgyysttiryqatgfgtneteylfevdnltyvqlesrftpqfllqlnetiytsgkrsnttgkliwkvnpeidttigewafwetkknltrkirseelsfaglitggrrtrreaivnaqpkcnpnlhywttqdegaaiglawipyfgpaaegiyieglmhnqdglicglrqlanettqalqlflrattelrtfsilnrkaidfllqrwggtchilgpdcciephdwtknitdkidqiihdfvdktlpdqgdndnwwtgwrqwipagigvtgviiavialfcickfvf'\n",
    "#\n",
    "#\n",
    "# # In[10]:\n",
    "#\n",
    "#\n",
    "# cont_val_ori.loc[cont_val_ori['id']=='EbolaGP','Antigen']='mgvtgilqlprdrfkrtsfflwviilfqrtfsiplgvihnstlqvsdvdklvcrdklsstnqlrsvglnlegngvatdvpsatkrwgfrsgvppkvvnyeagewaencynleikkpdgseclpaapdgirgfprcryvhkvsgtgpcagdfafhkegafflydrlastviyrgttfaegvvaflilpqakkdffsshplrepvnatedpssgyysttiryqatgfgtneteylfevdnltyvqlesrftpqfllqlnetiytsgkrsnttgkliwkvnpeidttigewafwetkknltrkirseelsfaglitggrrtrreaivnaqpkcnpnlhywttqdegaaiglawipyfgpaaegiyieglmhnqdglicglrqlanettqalqlflrattelrtfsilnrkaidfllqrwggtchilgpdcciephdwtknitdkidqiihdfvdktlpdqgdndnwwtgwrqwipagigvtgviiavialfcickfvf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414237e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# cont_val_ori.to_csv('/project/DPDS/Wang_lab/shared/BCR_antigen/data/train_validate/continuous_training/cont_train_cutoff.csv')\n",
    "## Replace Hie/EbolaGP's sequence to the short one\n",
    "# cont_val_ori.iloc[13,7]='IPLGVIHNSTLQVSDVDKLVCRDKLSSTNQLRSVGLNLEGNGVATDVPSATKRWGFRSGVPPKVVNYEAGEWAENCYNLEIKKPDGSECLPAAPDGIRGFPRCRYVHKVSGTGPCAGDFAFHKEGAFFLYDRLASTVIYRGTTFAEGVVAFLILPQAYSTTIRYQATGFGTNETEYLFEVDNLTYVQLESRFTPQFLLQLNETIYTSGKRSNTTGKLIWKVNRSEELSFS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e35a7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#cont_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a38764",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "column_names = ['id','Project','BetterBCR_Vh','BetterBCR_CDR3h','WorseBCR_Vh','WorseBCR_CDR3h','Antigen','type']\n",
    "cont_train_ori['type'] = 'continuous_training'\n",
    "#cont_train_libra['type'] = 'continuous_training_libra'\n",
    "input_ori['type'] = 'binary_training'\n",
    "exVal_ori['type'] = 'binary_validation'\n",
    "cont_val_ori['type'] = 'continuous_validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36f6f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "combine_training = pd.concat([input_ori[column_names],cont_train_ori[column_names]],axis=0)\n",
    "combine_validation = pd.concat([exVal_ori[column_names],cont_val_ori[column_names]],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d429f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('/project/DPDS/Wang_lab/shared/BCR_antigen/data/toInput/easy_V_dict.pkl','rb') as f:\n",
    "    Vh_dict = pickle.load(f)\n",
    "with open('/project/DPDS/Wang_lab/shared/BCR_antigen/data/toInput/easy_CDR3_dict.pkl','rb') as f:\n",
    "    CDR3h_dict = pickle.load(f)\n",
    "with open('/project/DPDS/Wang_lab/shared/BCR_antigen/data/toInput/hard_V_dict.pkl', 'rb') as f:\n",
    "    Vh_hard_dict = pickle.load(f)\n",
    "with open('/project/DPDS/Wang_lab/shared/BCR_antigen/data/toInput/hard_CDR3_dict.pkl', 'rb') as f:\n",
    "    CDR3h_hard_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b42e76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def functions\n",
    "\n",
    "def get_now():\n",
    "    now = str(datetime.now())\n",
    "    tosec = now.split('.')[0]\n",
    "    out_now = tosec.split(' ')[0]+'_'+tosec.split(' ')[1].replace(':',\"-\")\n",
    "    return out_now\n",
    "\n",
    "def filter_big_antigens(dataset,cutoff):\n",
    "    dataset['aalens']=list(map(len,dataset['Antigen'].tolist()))\n",
    "    data_filtered = dataset.loc[dataset['aalens']< cutoff]\n",
    "    print('After removing antigens larger than '+str(cutoff)+', '+str(100*data_filtered.shape[0]/dataset.shape[0])+'% antigens remained.')\n",
    "    return data_filtered\n",
    "\n",
    "def check_bad_bcr(seq):\n",
    "    allowed_letters = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    uppercase_string = seq.upper()\n",
    "    return not any(char not in allowed_letters for char in uppercase_string)\n",
    "\n",
    "def filter_bad_bcr(df,binary = True):\n",
    "    if binary:\n",
    "        mask = df[['BetterBCR_Vh', 'BetterBCR_CDR3h', 'WorseBCR_Vh', 'WorseBCR_CDR3h']].applymap(check_bad_bcr)\n",
    "    else:\n",
    "        mask = df[['BetterBCR_Vh', 'BetterBCR_CDR3h']].applymap(check_bad_bcr)\n",
    "    filtered_df = df[mask.all(axis=1)]\n",
    "    return filtered_df\n",
    "\n",
    "def preprocess(df,cutoff,binary = True):\n",
    "    if binary:\n",
    "        for col in df[['BetterBCR_Vh','BetterBCR_CDR3h','WorseBCR_Vh','WorseBCR_CDR3h','Antigen','type']].columns:\n",
    "            df[col] = df[col].apply(lambda x: x.replace(' ', ''))\n",
    "    else:\n",
    "        for col in df[['BetterBCR_Vh','BetterBCR_CDR3h','Antigen']].columns:\n",
    "            df[col] = df[col].apply(lambda x: x.replace(' ', ''))\n",
    "    df =  filter_bad_bcr(df,binary = binary)\n",
    "    df = filter_big_antigens(df,cutoff)\n",
    "    df = df.assign(antigen_index = df['Project'] + '/' + df['id'].astype(str))\n",
    "    df = df.sort_values('antigen_index',)\n",
    "    df = df.assign(record_id = ['record_' + str(s) for s in range(df.shape[0])])\n",
    "    return df\n",
    "\n",
    "# def has_space(string):\n",
    "#     return \" \" in string\n",
    "def check_bad_letters(df,binary = True):\n",
    "    if binary:\n",
    "        df = df[['BetterBCR_Vh','BetterBCR_CDR3h','WorseBCR_Vh','WorseBCR_CDR3h','Antigen']]\n",
    "    else:\n",
    "        df = df[['BetterBCR_Vh','BetterBCR_CDR3h','Antigen']]\n",
    "    for col in df.columns:\n",
    "        if not all(df[col].apply(check_bad_bcr)):\n",
    "            print(Fore.RED +str(col)+' contains uncommon aa or symbols!')\n",
    "        else:\n",
    "            print(Fore.GREEN +str(col)+' PASS!')\n",
    "    print(Style.RESET_ALL)\n",
    "\n",
    "def build_BCR_dict(dataset,colname,precise = False):\n",
    "    cols = dataset.filter(like = colname)\n",
    "    uniq_keys = pd.unique(cols.values.ravel()).tolist()\n",
    "    if colname == 'CDR3h':\n",
    "        uniq_embedding,_,input_keys = embedCDR3(uniq_keys,precise = precise)\n",
    "    elif colname == 'Vh':\n",
    "        uniq_embedding,_,input_keys = embedV(uniq_keys,precise = precise)\n",
    "    i = 0\n",
    "    mydict = {}\n",
    "    for key in input_keys:\n",
    "        mydict[key] = uniq_embedding[i]\n",
    "        i += 1\n",
    "    return(mydict)\n",
    "\n",
    "def predict_size(len,datatype = 'float32'):\n",
    "    # Define the shape of the tensor\n",
    "    shape = [1, len, len, 318]\n",
    "    element_size = np.dtype(datatype).itemsize\n",
    "    num_elements = math.prod(shape)\n",
    "    tensor_size = num_elements * element_size\n",
    "#    size_gb = tensor_size/(1024**3)\n",
    "#    print(f\"The tensor takes up {size_gb:.2f} GB of memory.\")\n",
    "#    size_gb = tensor_size/(1024*1024*1024)\n",
    "#    print(f\"Tensor size: {size_gb:.2f} GB\")\n",
    "    return tensor_size\n",
    "\n",
    "# for example, subsample is around 10000, each antigen 50 +-10;\n",
    "# df = input_ori\n",
    "def small_data(df,subsample,each = 50,around = 10):\n",
    "    subsample = min(subsample,df.shape[0])\n",
    "    df['antigen_index']= df['Project']+'/'+df['id']\n",
    "    antigen_ls = df['antigen_index'].unique().tolist()\n",
    "    num_to_pick = subsample//each\n",
    "    antigen_to_pick = random.sample(antigen_ls,min(num_to_pick,len(antigen_ls)))\n",
    "    #selected_types = random.sample(df['type'].unique().tolist(), min(num_types_to_select, len(df['type'].unique())))\n",
    "    # Initialize an empty dataframe to store the selected rows\n",
    "    selected_rows = pd.DataFrame()\n",
    "\n",
    "    # Loop through the selected types\n",
    "    for t in antigen_to_pick:\n",
    "        antigen_rows = df[df['antigen_index'] == t]\n",
    "\n",
    "        # Select 40-60 rows for each type, or all rows if there are not enough\n",
    "        num_rows_to_select = min(len(antigen_rows), random.randint(each-around, each+around))\n",
    "        selected_antigen_rows = antigen_rows.sample(num_rows_to_select)\n",
    "\n",
    "        # Append the selected rows to the final dataframe\n",
    "        selected_rows = pd.concat((selected_rows,selected_antigen_rows),axis=0)\n",
    "\n",
    "    # Reset the index of the final dataframe\n",
    "    selected_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print the final dataframe\n",
    "    return selected_rows\n",
    "\n",
    "def small_data_in(df,each = 3, around = 1):\n",
    "    df['antigen_index']= df['Project']+'/'+df['id']\n",
    "    df['good_pair']= df['antigen_index']+'_'+df['BetterBCR_Vh']+'_'+df['BetterBCR_CDR3h']\n",
    "    pair_list = df['good_pair'].unique().tolist()\n",
    "    selected_rows = pd.DataFrame()\n",
    "\n",
    "    # Loop through the selected types\n",
    "    for t in pair_list:\n",
    "        pair_rows = df[df['good_pair'] == t]\n",
    "\n",
    "        # Select 40-60 rows for each type, or all rows if there are not enough\n",
    "        num_rows_to_select = min(len(pair_rows), random.randint(each-around, each+around))\n",
    "        selected_pair_rows = pair_rows.sample(num_rows_to_select)\n",
    "\n",
    "        # Append the selected rows to the final dataframe\n",
    "        selected_rows = pd.concat((selected_rows,selected_pair_rows),axis=0)\n",
    "\n",
    "    # Reset the index of the final dataframe\n",
    "    selected_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print the final dataframe\n",
    "    return selected_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b388992",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_train_val_loader(dataloader,batchsize=BATCH_SIZE,validate=True):\n",
    "    train_dataset = dataloader.get_train_dataset()\n",
    "    train_loader = DataLoader(train_dataset, batchsize, sampler=GroupShuffleSampler(train_dataset, train_dataset.group_ids))\n",
    "    if validate:\n",
    "        val_dataset = dataloader.get_val_dataset()\n",
    "        val_loader = DataLoader(val_dataset, batchsize, sampler=GroupShuffleSampler(val_dataset, val_dataset.group_ids))\n",
    "        return train_loader,val_loader\n",
    "    else:\n",
    "        return train_loader\n",
    "#     if subsample:\n",
    "\n",
    "#     else:\n",
    "#         return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282202c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataloader = cont_loader_10x\n",
    "# val_dataset = dataloader.get_val_dataset()\n",
    "# val_loader = DataLoader(val_dataset, batchsize, sampler=GroupShuffleSampler(val_dataset, val_dataset.group_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e8722",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def generate_group_index(self, group_size):\n",
    "#     df = self.dataframe.copy()\n",
    "#     groups = df.groupby('antigen_index').cumcount() // group_size + 1\n",
    "#     df['group'] = groups\n",
    "#     df['group_id'] = df.apply(lambda row: row['antigen_index'] + '_' + str(row['group']), axis=1)\n",
    "#     group_id = df['group_id'].tolist()\n",
    "#     re turn group_id\n",
    "# df=c_10x.sample(frac=0.1)\n",
    "# groups = df.groupby('antigen_index').cumcount() // 50 + 1\n",
    "# df['group'] = groups\n",
    "# df['group_id'] = df.apply(lambda row: row['antigen_index'] + '_' + str(row['group']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee613c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# SampleDataset(train_cb,antigen_fpath_dict=NPY_DIR,cdr3_dict=CDR3h_hard_dict, v_dict= Vh_hard_dict, subsample_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5ba4c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def subsample_data(dataframe,subsample_ratio_1,subsample_ratio_2,by_project = False):#,num_samples=500):\n",
    "#     df = dataframe\n",
    "#     type_col='type'\n",
    "#     project_col='Project'\n",
    "#     id_col='antigen_index'\n",
    "#     type1_df = df[df[type_col].str.contains('^binary', na=False)]\n",
    "#     type2_df = df[df[type_col].str.contains('^continuous', na=False)]\n",
    "#     if not type1_df.empty:\n",
    "#         if subsample_ratio_1 < 1.0:\n",
    "#             type1_samples = []\n",
    "#             type2_samples = []\n",
    "#             # For type 1\n",
    "#             if by_project:\n",
    "#                 num_samples = round(df.shape[0]*subsample_ratio_1)//7\n",
    "#                 for project in type1_df[project_col].unique():\n",
    "#                     project_df = type1_df[type1_df[project_col] == project]\n",
    "#                     num_ids = project_df[id_col].nunique()\n",
    "#                     for id_ in project_df[id_col].unique():\n",
    "#                         id_df = project_df[project_df[id_col] == id_]\n",
    "#                         samples = id_df.sample(min(len(id_df), num_samples // num_ids), replace=False)\n",
    "#                         type1_samples.append(samples)\n",
    "#             else:\n",
    "#                 num_samples = round(df.shape[0]*subsample_ratio_1)//200\n",
    "#                 for id_ in type1_df[id_col].unique():\n",
    "#                     id_df = type1_df[type1_df[id_col] == id_]\n",
    "#                     samples = id_df.sample(min(len(id_df), num_samples), replace=False)\n",
    "#                     type1_samples.append(samples)\n",
    "#             if type1_samples:\n",
    "#                 type1_df = pd.concat(type1_samples)\n",
    "#             else:\n",
    "#                 type1_df = pd.DataFrame()\n",
    "\n",
    "# #         else:\n",
    "# #             return dataframe\n",
    "#     if not type1_df.empty:\n",
    "#         if subsample_ratio_2 < 1.0:\n",
    "#             # For type 2\n",
    "#             if by_project:\n",
    "#                 num_samples = round(df.shape[0]*subsample_ratio_2)//7\n",
    "#                 for project in type2_df[project_col].unique():\n",
    "#                     project_df = type2_df[type2_df[project_col] == project]\n",
    "#                     num_ids = project_df[id_col].nunique()\n",
    "#                     num_samples_project = num_samples // num_ids # if num_ids <= 5 else 100\n",
    "#                     for id_ in project_df[id_col].unique():\n",
    "#                         id_df = project_df[project_df[id_col] == id_]\n",
    "#                         samples = id_df.sample(min(len(id_df), num_samples_project), replace=False)\n",
    "#                         type2_samples.append(samples)\n",
    "#             else:\n",
    "#                 num_samples = round(df.shape[0]*subsample_ratio_2)//200\n",
    "#                 for id_ in type2_df[id_col].unique():\n",
    "#                     id_df = type2_df[type2_df[id_col] == id_]\n",
    "#                     samples = id_df.sample(min(len(id_df), num_samples), replace=False)\n",
    "#                     type2_samples.append(samples)\n",
    "#             if type2_samples:\n",
    "#                 type2_df = pd.concat(type2_samples)\n",
    "#             else:\n",
    "#                 type2_df = pd.DataFrame()\n",
    "\n",
    "#     resampled_df = pd.concat([type1_df,type2_df], ignore_index=True)\n",
    "#     return resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0f61f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##MARK HERE###\n",
    "\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, dataframe, antigen_fpath_dict, cdr3_dict, v_dict, subsample_ratio_1 = 1.0,subsample_ratio_2 = 1.0,by_project1=False,by_project2=False):\n",
    "        self.dataframe = self.subsample_data(dataframe, subsample_ratio_1,subsample_ratio_2,by_project1,by_project2)\n",
    "        self.group_ids = self.generate_group_index(group_size=GROUP_SIZE)  # Adjust the group_size as needed\n",
    "        self.your_data_list = self.get_my_data_list()\n",
    "        self.antigen_fpath_dict = antigen_fpath_dict\n",
    "#        self.antigen_fpath_dict = self.__read_files()\n",
    "#        self.your_data_list = your_data_list\n",
    "        self.antigen_dict = {}\n",
    "        self.cdr3_dict = cdr3_dict\n",
    "        self.v_dict = v_dict\n",
    "        self.antigen_in = {}\n",
    "        self.lens_dict = {}\n",
    "\n",
    "    def subsample_data(self,dataframe,subsample_ratio_1,subsample_ratio_2,by_project1,by_project2):#,num_samples=500):\n",
    "        df = dataframe\n",
    "        type_col='type'\n",
    "        project_col='Project'\n",
    "        id_col='antigen_index'\n",
    "        type1_df = df[df[type_col].str.contains('^binary', na=False)]\n",
    "        type2_df = df[df[type_col].str.contains('^continuous', na=False)]\n",
    "        if not type1_df.empty:\n",
    "            if subsample_ratio_1 < 1.0:\n",
    "                type1_samples = []\n",
    "                # For type 1\n",
    "                if by_project1:\n",
    "                    num_samples = round(df.shape[0]*subsample_ratio_1)//7\n",
    "                    for project in type1_df[project_col].unique():\n",
    "                        project_df = type1_df[type1_df[project_col] == project]\n",
    "                        num_ids = project_df[id_col].nunique()\n",
    "                        for id_ in project_df[id_col].unique():\n",
    "                            id_df = project_df[project_df[id_col] == id_]\n",
    "                            samples = id_df.sample(min(len(id_df), num_samples // num_ids), replace=False)\n",
    "                            type1_samples.append(samples)\n",
    "                else:\n",
    "                    num_samples = round(df.shape[0]*subsample_ratio_1)//50\n",
    "                    for id_ in type1_df[id_col].unique():\n",
    "                        id_df = type1_df[type1_df[id_col] == id_]\n",
    "                        samples = id_df.sample(min(len(id_df), num_samples), replace=False)\n",
    "                        type1_samples.append(samples)\n",
    "                if type1_samples:\n",
    "                    type1_df = pd.concat(type1_samples)\n",
    "                else:\n",
    "                    type1_df = pd.DataFrame()\n",
    "\n",
    "#         else:\n",
    "#             return dataframe\n",
    "        if not type2_df.empty:\n",
    "            if subsample_ratio_2 < 1.0:\n",
    "                type2_samples = []\n",
    "                # For type 2\n",
    "                if by_project2:\n",
    "                    num_samples = round(df.shape[0]*subsample_ratio_2)//7\n",
    "                    for project in type2_df[project_col].unique():\n",
    "                        project_df = type2_df[type2_df[project_col] == project]\n",
    "                        num_ids = project_df[id_col].nunique()\n",
    "                        num_samples_project = num_samples // num_ids # if num_ids <= 5 else 100\n",
    "                        for id_ in project_df[id_col].unique():\n",
    "                            id_df = project_df[project_df[id_col] == id_]\n",
    "                            samples = id_df.sample(min(len(id_df), num_samples_project), replace=False)\n",
    "                            type2_samples.append(samples)\n",
    "                else:\n",
    "                    num_samples = round(df.shape[0]*subsample_ratio_2)//50\n",
    "                    for id_ in type2_df[id_col].unique():\n",
    "                        id_df = type2_df[type2_df[id_col] == id_]\n",
    "                        samples = id_df.sample(min(len(id_df), num_samples), replace=False)\n",
    "                        type2_samples.append(samples)\n",
    "                if type2_samples:\n",
    "                    type2_df = pd.concat(type2_samples)\n",
    "                else:\n",
    "                    type2_df = pd.DataFrame()\n",
    "\n",
    "        resampled_df = pd.concat([type1_df,type2_df], ignore_index=True)\n",
    "        return resampled_df\n",
    "\n",
    "\n",
    "\n",
    "#     def subsample_data(self, dataframe, subsample_ratio):\n",
    "#         if subsample_ratio < 1.0:\n",
    "#             return dataframe.sample(frac=subsample_ratio)\n",
    "#         else:\n",
    "#             return dataframe\n",
    "    def __getitem__(self, idx):\n",
    "        your_dict = self.your_data_list[idx]\n",
    "        antigen_key = your_dict['antigen_index']\n",
    "#        print(antigen_key)\n",
    "        aalens_key = your_dict['aalens']\n",
    "        self.lens_dict[antigen_key]=aalens_key\n",
    "        betterCDR_key = your_dict['BetterBCR_CDR3h']\n",
    "        betterV_key = your_dict['BetterBCR_Vh']\n",
    "        index_key = your_dict['record_id']\n",
    "        better_bcr = self.__embedding_BCR(betterCDR_key,betterV_key,precise = True)\n",
    "        self.__get_antigen_in(antigen_key)\n",
    "        antigen_feat = self.antigen_in[antigen_key]\n",
    "#        print('antigen shape',self.antigen_in[antigen_key].shape)\n",
    "        better_pair = self.__comb_embed_gpu(antigen_key,better_bcr)\n",
    "        if any('Worse' in key for key in your_dict):\n",
    "            worseCDR_key = your_dict['WorseBCR_CDR3h']\n",
    "            worseV_key = your_dict['WorseBCR_Vh']\n",
    "            worse_bcr = self.__embedding_BCR(worseCDR_key,worseV_key,precise = True)\n",
    "            worse_pair = self.__comb_embed_gpu(antigen_key,worse_bcr)\n",
    "            return better_pair, worse_pair, index_key, antigen_key#better_feat, worse_feat,\n",
    "        else:\n",
    "            score = your_dict['Score']\n",
    "            return better_pair, score, index_key, antigen_key#better_feat, worse_feat,\n",
    "\n",
    "#        self.lens_dict[antigen_key] = aalens_key\n",
    "#        antigen_feat = self.__extract_antigen(antigen_key)\n",
    "        ##check whether antigen_index in antigen_in; if not, import it to\n",
    "\n",
    "\n",
    "#        print('better shape',better_feat.shape)\n",
    "#         better_pair = self.__comb_embed(antigen_key,better_feat)\n",
    "#         worse_pair = self.__comb_embed(antigen_key,worse_feat)\n",
    "\n",
    "\n",
    "#        better_out = better_pair.squeeze(dim=0)\n",
    "#        worse_out = worse_pair.squeeze(dim=0)\n",
    "        # print('better out dtype:',better_out.dtype)\n",
    "        # print('worse out dtype:',worse_out.dtype)\n",
    "#        out_dict = {}\n",
    "#        out_dict[index_key] = (better_out, worse_out)\n",
    "#        return better_out.to(device), worse_out.to(device)#, aalen_key #IF RUN COMBINING_EMBED in cpu\n",
    "    #IF RUN COMBINING_EMBED in GPU:\n",
    "\n",
    "    def get_my_data_list(self,selected_cols = 'BCR_Vh|BCR_CDR3h|Antigen|aalens|antigen_index|record_id|Score'):\n",
    "        ds_to_dict = self.dataframe.filter(regex=selected_cols)\n",
    "    #ds_to_dict = dataset[selected_cols].set_index('record_id')\n",
    "        my_data_list = ds_to_dict.to_dict(orient='records')\n",
    "        return my_data_list\n",
    "\n",
    "    def generate_group_index(self, group_size):\n",
    "        df = self.dataframe.copy()\n",
    "        groups = df.groupby('antigen_index').cumcount() // group_size + 1\n",
    "        df['group'] = groups\n",
    "        df['group_id'] = df.apply(lambda row: row['antigen_index'] + '_' + str(row['group']), axis=1)\n",
    "        group_id = df['group_id'].tolist()\n",
    "        return group_id\n",
    "\n",
    "    def __comb_embed_gpu(self,antigen_name,BCR_feat):\n",
    "#        print('antigen to comb:',antigen_name)\n",
    "        lengthen = CHANNEL_ANTIGEN\n",
    "        #rint('The current antigen is: ',antigen_name)\n",
    "#        print('length get from dict_len:',lengthen)\n",
    "        single_antigen_g = self.antigen_in[antigen_name][0]\n",
    "#        single_antigen_g = F.normalize(single_antigen_g, p=2, dim=3)\n",
    "#        print('shape of antigen from antigen dict:',single_antigen_g.shape)\n",
    "#        single_antigen_g = torch.from_numpy(single_antigen).to(device)\n",
    "        single_BCR_g = torch.from_numpy(BCR_feat).to(device)\n",
    "#        print('single BCR shape:',single_BCR_g.shape)\n",
    "#        single_BCR_g = torch.from_numpy(BCR_feat).half().to(device)\n",
    "        BCR_t = torch.tile(single_BCR_g,(lengthen,lengthen,1))\n",
    "#        print('tiled bcr shape',BCR_t.shape)\n",
    "#        print('shape of BCR_tiled:',BCR_t.shape)#empty\n",
    "        pair_feat_g = torch.cat((single_antigen_g,BCR_t),dim=2)\n",
    "        del single_BCR_g,BCR_t\n",
    "        torch.cuda.empty_cache()\n",
    "        return pair_feat_g#.half()\n",
    "#     def __comb_embed(self,single_antigen,BCR_feat):\n",
    "#         '''process your data'''\n",
    "#     #    singe_antigen = antigen_dict[antigen_name]\n",
    "#         lengthen = aalen(single_antigen)\n",
    "# #         if verbose ==True:\n",
    "# #             if lengthen > 500:\n",
    "# #                 print(Fore.RED + 'length of antigen over 500: '+str(lengthen))\n",
    "# #                 print(Style.RESET_ALL)\n",
    "#         BCR_tile = np.tile(BCR_feat,(1,lengthen,lengthen,1))\n",
    "#         pair_np = np.concatenate((single_antigen,BCR_tile),axis=3)\n",
    "#         pair_feat = torch.from_numpy(pair_np)\n",
    "#         return(pair_feat)\n",
    "\n",
    "    def __get_antigen_in(self,antigen_name):\n",
    "        #print('Next antigen:',antigen_name)\n",
    "        if not antigen_name in self.antigen_in:\n",
    "            if not antigen_name in self.antigen_dict:\n",
    "                antigen_to_in = self.extract_antigen(antigen_name)\n",
    "            else:\n",
    "                antigen_to_in = self.antigen_dict[antigen_name]\n",
    "            ##check and import\n",
    "            self.__rotate_dict_in(antigen_name,limit_size = LIMIT)\n",
    "            #single_antigen_in =\n",
    "            try:\n",
    "\n",
    "                antigen_tensor = torch.from_numpy(antigen_to_in).to(device)\n",
    "                self.antigen_in[antigen_name] = self.pool_antigen(antigen_tensor,CHANNEL_ANTIGEN) ###ON CPU\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(\"CUDA out of memory. Clearing cache and trying again...\")\n",
    "                    for key in self.antigen_in:\n",
    "                        self.antigen_in.pop(key, None)\n",
    "                    self.antigen_in.clear()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # Try the operation again\n",
    "                    try:\n",
    "                        self.antigen_in[antigen_name] = torch.from_numpy(antigen_to_in).to(device)\n",
    "                    except RuntimeError as e:\n",
    "                        if \"CUDA out of memory\" in str(e):\n",
    "                            print(\"Still out of memory after clearing cache.\")\n",
    "                        else:\n",
    "                            raise\n",
    "                else:\n",
    "                    raise\n",
    "#            self.antigen_in[antigen_name] = torch.from_numpy(antigen_to_in).half().to(device)\n",
    "\n",
    "#        return single_antigen_in\n",
    "    def __rotate_dict_in(self,antigen_name,limit_size=LIMIT,verbose=False):\n",
    "#        while self.__check_size(antigen_name,limit_size = LIMIT) and len(self.antigen_in)>0:\n",
    "        while self.__check_size(antigen_name,limit_size = LIMIT,datatype = 'float32') and len(self.antigen_in)>0 or len(self.antigen_in)> Max_num:\n",
    "\n",
    "            key = random.choice(list(self.antigen_in.keys()))\n",
    "#            print(self.antigen_in.keys())\n",
    "            self.antigen_in.pop(key, None)\n",
    "            if verbose:\n",
    "                print(Fore.RED +'the antigen:'+str(key)+'is deleted!'+ Fore.RESET)\n",
    "            torch.cuda.empty_cache()\n",
    "#        if len(self.antigen_in) >3:\n",
    "            if verbose:\n",
    "                print(Fore.RED +'antigens still in:'+str(self.antigen_in.keys())+ Fore.RESET)\n",
    "\n",
    "    def pool_antigen(self,antigen_input,out_n_channel):\n",
    "#        lengthen = antigen_input.shape[1]\n",
    "        pooling_layer = nn.AdaptiveAvgPool2d((out_n_channel,out_n_channel))\n",
    "        output = pooling_layer(antigen_input.permute(0,3,1,2)).permute(0,2,3,1)\n",
    "        return output\n",
    "    def __check_size(self,antigen_name,limit_size= LIMIT,datatype = 'float32'):\n",
    "        dict_size = 0\n",
    "        for key in self.antigen_in:\n",
    "    #        print(key,\":\")\n",
    "            aalen = self.lens_dict[key]\n",
    "    #        print(aalen)\n",
    "            dict_size += predict_size(aalen,datatype = datatype)\n",
    "    #    print('size in dict:',dict_size)\n",
    "        check = dict_size + predict_size(self.lens_dict[antigen_name])\n",
    "        check_limit = limit_size*(1024**3)\n",
    "    #    print('size all to check:',check_size)\n",
    "        return check > check_limit\n",
    "\n",
    "\n",
    "    def extract_antigen(self,antigen_name,verbose=False):\n",
    "        if antigen_name in self.antigen_dict:\n",
    "            single_antigen = self.antigen_dict[antigen_name]\n",
    "        else:\n",
    "            try:\n",
    "                antigen_import = np.load(str(self.antigen_fpath_dict+'/'+antigen_name+'.pair.npy'))/w\n",
    "                if not antigen_import.shape[1] == self.lens_dict[antigen_name]:\n",
    "                    print(Fore.RED + 'antigen ' +str(antigen_name)+' embedding '+str(antigen_import.shape[1])+' is NOT in the correct shape '+str(self.lens_dict[antigen_name])+'!'+ Style.RESET_ALL)\n",
    "                    exit()\n",
    "                single_antigen = antigen_import\n",
    "#                single_antigen = torch.from_numpy(antigen_import).to(device) ###ON GPU\n",
    "#            print(npy.shape)\n",
    "                self.antigen_dict[antigen_name] = single_antigen\n",
    "                if verbose:\n",
    "                    print(Fore.RED + 'New antigen added to dictionary:'+antigen_name+Fore.RESET)\n",
    "            except ValueError:\n",
    "                print('The embedding of antigen %s cannot be found!' % antigen_name)\n",
    "#             if verbose:\n",
    "#                 print(Fore.RED + 'New antigen added to dictionary:',antigen_name)\n",
    "#                 print(Fore.RED + 'Number of antigens included:',len(self.antigen_dict))\n",
    "#                 print(Style.RESET_ALL)\n",
    "        return single_antigen\n",
    "\n",
    "    def __embedding_BCR(self,cdr3_seq,v_seq,precise = False):\n",
    "        if cdr3_seq not in self.cdr3_dict:\n",
    "#            print('CDR3 not in dictionary!!')\n",
    "#            df1 = pd.DataFrame()\n",
    "            cdr3_feat,*_ = embedCDR3([cdr3_seq],precise = precise)\n",
    "            cdr3_feat = cdr3_feat[0]\n",
    "            self.cdr3_dict[cdr3_seq]=cdr3_feat\n",
    "        else:\n",
    "#            print('CDR3 in dictionary!!')\n",
    "            cdr3_feat = self.cdr3_dict[cdr3_seq]\n",
    "        if v_seq not in self.v_dict:\n",
    "#            print('V not in dictionary!!')\n",
    "#            df2 = pd.DataFrame([v_seq])\n",
    "            v_feat,*_ = embedV([v_seq],precise = precise)\n",
    "            v_feat = v_feat[0]\n",
    "            self.v_dict[v_seq]=v_feat\n",
    "        else:\n",
    "#            print('V in dictionary!!')\n",
    "            v_feat = self.v_dict[v_seq]\n",
    "        bcr_feat = np.concatenate((cdr3_feat,v_feat))\n",
    "        return bcr_feat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.your_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a6363",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_data_list(df,selected_cols = 'BCR_Vh|BCR_CDR3h|Antigen|aalens|antigen_index|record_id|Score'):\n",
    "#     ds_to_dict = df.filter(regex=selected_cols)\n",
    "# #ds_to_dict = dataset[selected_cols].set_index('record_id')\n",
    "#     my_data_list = ds_to_dict.to_dict(orient='records')\n",
    "#     return my_data_list\n",
    "# c10_dict = get_data_list(c_10x)\n",
    "# #selected_10X= c_10x.filter(regex='BCR_Vh|BCR_CDR3h|Antigen|aalens|antigen_index|record_id|Score')\n",
    "#selected_10X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b1486",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GroupShuffleSampler(Sampler):\n",
    "    def __init__(self, data_source, group_ids, shuffle=True):\n",
    "        self.data_source = data_source\n",
    "        self.group_ids = group_ids\n",
    "        self.group_indices = self._group_indices()\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def _group_indices(self):\n",
    "        \"\"\"b\n",
    "        Returns a dictionary where the keys are the unique group_ids and\n",
    "        the values are the indices corresponding to each group_id in the data_source.\n",
    "        \"\"\"\n",
    "        group_indices = {}\n",
    "        for idx, group_id in enumerate(self.group_ids):\n",
    "            if group_id not in group_indices:\n",
    "                group_indices[group_id] = []\n",
    "            group_indices[group_id].append(idx)\n",
    "        return group_indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Generates an iterator that yields indices of the data_source\n",
    "        \"\"\"\n",
    "        group_ids = list(self.group_indices.keys())\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(group_ids)\n",
    "        indices = []\n",
    "        for group_id in group_ids:\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.group_indices[group_id])\n",
    "            indices += self.group_indices[group_id]\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "class InLoader:\n",
    "    def __init__(self, dataframe, antigen_fpath_dict, cdr3_dict, v_dict,train_ratio=0.8, subsample_ratio1=.01,subsample_ratio2=0.01, by_project1=False,by_project2=False,seed=SEED):\n",
    "        self.dataframe = dataframe\n",
    "        self.train_ratio = train_ratio\n",
    "        self.subsample_ratio1 = subsample_ratio1\n",
    "        self.subsample_ratio2 = subsample_ratio2\n",
    "        self.by_project1=by_project1\n",
    "        self.by_project2=by_project2\n",
    "#        self.subsampled = self.subsample()\n",
    "        self.seed = seed\n",
    "        self.antigen_fpath_dict = antigen_fpath_dict\n",
    "        self.cdr3_dict = cdr3_dict\n",
    "        self.v_dict = v_dict\n",
    "#        self.lens_dict = lens_dict\n",
    "        self.train_df, self.val_df = self.split_data()\n",
    "\n",
    "    def split_data(self):\n",
    "        if self.dataframe['type'].nunique()==1:\n",
    "            train_df, val_df = train_test_split(self.dataframe, train_size=self.train_ratio, random_state=self.seed)\n",
    "        else:\n",
    "            binary = self.dataframe[self.dataframe['type'] == 'binary_training']\n",
    "            cont = self.dataframe[self.dataframe['type'] == 'continuous_training']\n",
    "            binary_train_df, binary_val_df = train_test_split(\n",
    "                binary, train_size=self.train_ratio, random_state=self.seed)\n",
    "            cont_train_df, cont_val_df = train_test_split(\n",
    "                cont, train_size=self.train_ratio, random_state=self.seed)\n",
    "            train_df = pd.concat([binary_train_df,cont_train_df],axis=0)\n",
    "            val_df = pd.concat([binary_val_df,cont_val_df],axis = 0)\n",
    "        return train_df, val_df\n",
    "\n",
    "    def get_train_dataset(self):\n",
    "        return SampleDataset(self.train_df, self.antigen_fpath_dict, self.cdr3_dict, self.v_dict, subsample_ratio_1=self.subsample_ratio1, subsample_ratio_2 = self.subsample_ratio2,by_project1= self.by_project1,by_project2= self.by_project2)\n",
    "    def get_val_dataset(self):\n",
    "        return SampleDataset(self.val_df, self.antigen_fpath_dict, self.cdr3_dict, self.v_dict, subsample_ratio_1=self.subsample_ratio1,subsample_ratio_2 = self.subsample_ratio2,by_project1= self.by_project1,by_project2= self.by_project2)\n",
    "\n",
    "    def new_epoch(self):\n",
    "#        print(\"Starting a new epoch...\")\n",
    "        self.train_df, self.val_df = self.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf7b50",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# def subsample(df,  num_samples=500):\n",
    "#     type_col='type'\n",
    "#     project_col='Project'\n",
    "#     id_col='id'\n",
    "#     type1_df = df[df[type_col] == 'binary_training']\n",
    "#     type2_df = df[df[type_col] == 'continuous_training']\n",
    "\n",
    "#     type1_samples = []\n",
    "#     type2_samples = []\n",
    "\n",
    "#     # For type 1\n",
    "#     for project in type1_df[project_col].unique():\n",
    "#         project_df = type1_df[type1_df[project_col] == project]\n",
    "#         num_ids = project_df[id_col].nunique()\n",
    "#         for id_ in project_df[id_col].unique():\n",
    "#             id_df = project_df[project_df[id_col] == id_]\n",
    "#             samples = id_df.sample(min(len(id_df), num_samples // num_ids), replace=False)\n",
    "#             type1_samples.append(samples)\n",
    "\n",
    "#     # For type 2\n",
    "#     for project in type2_df[project_col].unique():\n",
    "#         project_df = type2_df[type2_df[project_col] == project]\n",
    "#         num_ids = project_df[id_col].nunique()\n",
    "#         num_samples_project = num_samples // num_ids # if num_ids <= 5 else 100\n",
    "#         for id_ in project_df[id_col].unique():\n",
    "#             id_df = project_df[project_df[id_col] == id_]\n",
    "#             samples = id_df.sample(min(len(id_df), num_samples_project), replace=False)\n",
    "#             type2_samples.append(samples)\n",
    "\n",
    "#     resampled_df = pd.concat(type1_samples + type2_samples, ignore_index=True)\n",
    "\n",
    "#     return resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834b01e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of SelfAttentionPooling\n",
    "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
    "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim,hidden_dim):\n",
    "        super(SelfAttentionPooling, self).__init__()\n",
    "#        hidden_dim=10\n",
    "        self.W1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.W2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "\n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "\n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "        att_w = F.softmax(self.W2(self.relu(self.W1(batch_rep))).squeeze(-1),dim=1).unsqueeze(-1)\n",
    "\n",
    "        utter_rep = torch.sum(batch_rep* att_w, dim=1)\n",
    "\n",
    "        return utter_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243b9a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###MODEL\n",
    "\n",
    "class mix_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mix_model,self).__init__()\n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Linear(318,40),#.to(torch.float64),\n",
    "            # in (1,len,len,318)\n",
    "            # out (1,len,len.50)\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(40,30),#.to(torch.float64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(30,20),#.to(torch.float64),\n",
    "            # out (1,len,len,20)\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.model2 = SelfAttentionPooling(input_dim=20,hidden_dim=30)\n",
    "        self.model2_1 = SelfAttentionPooling(input_dim=20,hidden_dim=30)\n",
    "        # input_dim = hidden size (number of channels)\n",
    "#        self.model2 = nn.MultiheadAttention(embed_dim=20,num_heads=N_HEADS,batch_first=True)\n",
    "        self.model3 = nn.Sequential(\n",
    "            nn.Linear(20,15),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(15,1)\n",
    "        )\n",
    "#     def __init__(self):\n",
    "#         super(mix_model,self).__init__()\n",
    "#         self.model1 = nn.Sequential(\n",
    "#             nn.Linear(318,40),#.to(torch.float64),\n",
    "#             # in (1,len,len,318)\n",
    "#             # out (1,len,len.50)\n",
    "#             nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(40,30),#.to(torch.float64),\n",
    "#             nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(30,20),#.to(torch.float64),\n",
    "#             # out (1,len,len,20)\n",
    "#             nn.LeakyReLU(0.1)\n",
    "#         )\n",
    "#         self.model2 = SelfAttentionPooling(input_dim=20,hidden_dim=30)\n",
    "#         self.model2_1 = SelfAttentionPooling(input_dim=20,hidden_dim=30)\n",
    "#         # input_dim = hidden size (number of channels)\n",
    "# #        self.model2 = nn.MultiheadAttention(embed_dim=20,num_heads=N_HEADS,batch_first=True)\n",
    "#         self.model3 = nn.Sequential(\n",
    "#             nn.Linear(20,15),\n",
    "#             nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(15,1)\n",
    "#         )\n",
    "        self.alpha1 = nn.Parameter(torch.randn(1))\n",
    "        self.beta1 = nn.Parameter(torch.randn(1))\n",
    "        self.alpha2 = nn.Parameter(torch.randn(1))\n",
    "        self.beta2 = nn.Parameter(torch.randn(1))\n",
    "#        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,x,binary = True, is_10X = True): ###because in getitem, return is .cuda(), now input is on gpu\n",
    "#         x = torch.empty(0)\n",
    "#         x = x.to(device)\n",
    "#        x = x.permute(0,2,1,3)\n",
    "#         print('after permute',x.shape)\n",
    "        x = self.model1(x)\n",
    "#         print('after model1',x.shape)\n",
    "        x0 = torch.empty(0)\n",
    "        x0 = x0.to(device)\n",
    "        for i in range(len(x)):\n",
    "            k = x[i]\n",
    "            k = self.model2(k).unsqueeze(0)\n",
    "#             print('after model2',k.shape)\n",
    "            k = self.model2_1(k)\n",
    "#             print('after model2_1',k.shape)\n",
    "            x0 = torch.cat((x0, k), dim=0)\n",
    "#         print('after loop:',x0.shape)\n",
    "        x0 = F.normalize(x0)\n",
    "        x0 = self.model3(x0).squeeze()\n",
    "        if binary:\n",
    "            out  = x0\n",
    "        else:\n",
    "            if is_10X:\n",
    "                out = x0*(-math.exp(self.alpha1))+self.beta1\n",
    "            else:\n",
    "                out = x0*(-math.exp(self.alpha2))+self.beta2\n",
    "        return(out)\n",
    "\n",
    "# model_b = mix_model(binary = True)\n",
    "# model_10x = mix_model(binary = False, is_10X = True)\n",
    "# model_libra = mix_model(binary = False, is_10X = False)\n",
    "# #model_mix\n",
    "# #model_mix.to(device)\n",
    "# #checkpoint = torch.load('/project/DPDS/Wang_lab/shared/BCR_antigen/data/output/2023-02-18_12-33-25_tag2/model_comb/Batch50BatchNumber_600Lr0.01Epoch54tag2_easy_neg.pth')\n",
    "# if MODEL is not None:\n",
    "#     checkpoint = torch.load('/project/DPDS/Wang_lab/shared/BCR_antigen/data/output/'+ MODEL)\n",
    "#     model_mix.load_state_dict(checkpoint)\n",
    "\n",
    "# model_b.to(device)\n",
    "# model_10x.to(device)\n",
    "# model_libra.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58c7ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(out_b,out_w,binary = True):\n",
    "    if not binary:\n",
    "        loss = torch.sum(torch.pow((out_b-out_w),2)) #out_w is the score given, out_b is the score predict\n",
    "    else:\n",
    "        loss = torch.sum(torch.relu(out_b-out_w+ T)+LAMBDA*(out_b*out_b + out_w*out_w))\n",
    "    return loss/out_b.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c301a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def select_model(model_class,device,binary = True, is_10X = True):\n",
    "#     model = model_class(binary= binary, is_10X = is_10X)\n",
    "#     model.to(device)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8b5a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_result(batch,model,loss_fn,binary = True,is_10X=True):\n",
    "#    print(zip(index_idx,antigen_index))\n",
    "#    print(b_pair.shape,w_pair.shape)\n",
    "    b_pair,w_pair,index_idx,antigen_index =batch ##Change the InLoader, when binary is False, w_pair = score\n",
    "    out_b = model(b_pair,binary = binary, is_10X = is_10X)\n",
    "    if binary:\n",
    "        out_w = model(w_pair)\n",
    "        loss = loss_fn(out_b,out_w,binary = binary)\n",
    "        outb = out_b.tolist()\n",
    "        outw = out_w.tolist()\n",
    "        success = torch.gt(out_w,out_b).tolist()\n",
    "        df = pd.DataFrame({'record_id':index_idx,'antigen':antigen_index,'out_b':outb,'out_w':outw,'success':success})\n",
    "    else:\n",
    "#         b_pair,score,index_idx,antigen_index =batch ##Change the InLoader, when binary is False, w_pair = score\n",
    "#         predict = model(b_pair,binary = bin)\n",
    "        out_score = w_pair.to(device)\n",
    "        loss = loss_fn(out_b,out_score,binary = binary)\n",
    "        pred = out_b.tolist()\n",
    "        score = out_score.tolist()\n",
    "        df = pd.DataFrame({'record_id':index_idx,'antigen':antigen_index,'predicted':pred,'score':w_pair,'success':np.nan})\n",
    "    return(df,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b59366",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for batch in c10_train_loader:\n",
    "#     b_pair,w_pair,index_idx,antigen_index =batch ##Change the InLoader, when binary is False, w_pair = score\n",
    "#     out_b = model_mix(b_pair,binary = False, is_10X = True)\n",
    "#     score = w_pair.to(device)\n",
    "#     loss = torch.pow((out_b - score), 2)\n",
    "#     print(loss)\n",
    "# #     df,loss = batch_result(batch,model_mix,loss_function,binary=False)\n",
    "# #    print(loss)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d3f1d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for batch in c10_val_loader:\n",
    "#     b_pair,w_pair,index_idx,antigen_index =batch\n",
    "#     print(index_idx,antigen_index,w_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b48ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# c10_train_loader = gen_train_val_loader(cont_loader_10x,binary = False)\n",
    "# for batch in c10_train_loader:\n",
    "#     b_pair,w_pair,index_idx,antigen_index =batch\n",
    "# #    print(b_pair.shape)\n",
    "#     print(w_pair)\n",
    "#     break\n",
    "    ###MARK HERE :internal validation cannot be done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae00ff0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for name, param in model_mix.named_parameters():\n",
    "# #    print(name, param.dtype)\n",
    "# def generate_optimizer(model,lr=LR):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode = 'min',factor=0.1,patience =5,verbose=False,threshold_mode='rel',threshold=1e-4)\n",
    "#     return optimizer,scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9ea52",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(dataloader,model,loss_fn,optimizer,clip_value,verbose = False,binary = True, is_10X = True):\n",
    "    train_loss = 0.0\n",
    "    i = 0\n",
    "    res = pd.DataFrame(columns=['record_id', 'antigen', 'out_b','out_w','success'])\n",
    "#    model = select_model(model_class,device=device,binary = binary, is_10X = is_10X)\n",
    "#     optimizer,scheduler = generate_optimizer(model,lr=LR)\n",
    "    model.train()\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        df,loss=batch_result(batch,model,loss_fn,binary = binary,is_10X=is_10X) ###MARK HERE\n",
    "        res = pd.concat([res,df],axis=0,ignore_index=True)\n",
    "        train_loss += loss.item()\n",
    "#         if binary:\n",
    "#             accu_biased = sum(res['success'])/res.shape[0]\n",
    "#             grouped = res.groupby('antigen')['success'].mean()\n",
    "#             accu_unbiased = grouped.mean()\n",
    "#             if verbose:\n",
    "#                 print('Batch',i,'batch_loss',loss.item(),'loss_until_now',train_loss/i,'accu_biased',accu_biased,'accu_unbiased',accu_unbiased)\n",
    "#         else:\n",
    "        if verbose:\n",
    "            print('Batch',i,'batch_loss',loss.item(),'loss_until_now',train_loss/i)\n",
    "        i += 1\n",
    "        if math.isinf(loss) or math.isnan(loss):\n",
    "            prob_bw = [out_b.item(),out_w.item()]\n",
    "            print('ERROR: The loss is INF or NaN! '+str(prob_bw))\n",
    "            break\n",
    "        loss.backward()\n",
    "        if clip_value is not None:\n",
    "            nn.utils.clip_grad_value_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "#     return train_loss/i,accu_biased,accu_unbiased,res,grouped\n",
    "    return train_loss/i,res\n",
    "\n",
    "def val_epoch(dataloader,ds,model,loss_fn,verbose = False):\n",
    "    val_loss = 0.0\n",
    "    i = 0\n",
    "    res = pd.DataFrame(columns=['record_id', 'antigen', 'out_b','out_w','success'])\n",
    "#    model = select_model(model_class,device=device,binary = True)\n",
    "#    optimizer,scheduler = generate_optimizer(model,lr=LR)\n",
    "    model.eval()\n",
    "    for batch in tqdm(dataloader):\n",
    "        df,loss=batch_result(batch,model,loss_fn)\n",
    "        res = pd.concat([res,df],axis=0,ignore_index=True)\n",
    "        res['type'] = res['record_id'].map(ds.set_index('record_id')['type'])\n",
    "        val_loss += loss.item()\n",
    "        accu_biased = sum(res['success'])/res.shape[0]\n",
    "        grouped = res.groupby(['antigen','type'])['success'].mean()\n",
    "        accu_unbiased = grouped.mean()\n",
    "        i += 1\n",
    "        if verbose:\n",
    "            print('Batch',i,'batch_loss',loss.item(),'loss_until_now',val_loss/i,'accu_biased',accu_biased,'accu_unbiased',accu_unbiased)\n",
    "        if math.isinf(loss) or math.isnan(loss):\n",
    "            prob_bw = [out_b.tolist(),out_w.tolist()]\n",
    "            print('ERROR: The loss is INF or NaN! '+str(prob_bw))\n",
    "            break\n",
    "    return val_loss/i,accu_biased,accu_unbiased,res,grouped\n",
    "\n",
    "# def summary_cohort(*dfs):\n",
    "#     if len(dfs) == 1:\n",
    "#         df = dfs[0]\n",
    "#     elif len(dfs) > 1:\n",
    "#         df = pd.concat(dfs)\n",
    "#     else:\n",
    "#         print('No Dataframe provided!')\n",
    "#         exit()\n",
    "#     df[['cohort','antigen']]= df['antigen'].str.split('/',1,expand = True)\n",
    "#     return df.groupby('cohort')['success'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb0c7d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# combine_training['Project'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3847952",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make a function to subsample training data in a balance way#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea125a6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# combine_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710751e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# EACH = 100\n",
    "# Small_sample = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6553715",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "if Small_sample is not None:\n",
    "#    binary_train = small_data(input_ori,subsample=Small_sample,each = EACH)\n",
    "#    binary_val = small_data(exVal_ori,subsample=Small_sample,each = EACH)\n",
    "#    cont_train = small_data(cont_train_ori,subsample=Small_sample,each = EACH)\n",
    "    train_cb = small_data(combine_training,subsample=Small_sample,each = EACH)\n",
    "    val_cb = small_data(combine_validation,subsample=Small_sample,each = EACH)\n",
    "\n",
    "#    con_val = small_data(cont_val_ori,subsample=Small_sample,each=EACH)\n",
    "else:\n",
    "#    binary_train = input_ori\n",
    "#    binary_val = exVal_ori\n",
    "#     train_cb = small_data(combine_training,subsample=combine_training.shape[0],each = EACH)\n",
    "#     val_cb = small_data(combine_validation,subsample=combine_validation.shape[0],each = EACH)\n",
    "#    cont_train = cont_train_ori\n",
    "    val_cb = combine_validation\n",
    "    train_cb = combine_training\n",
    "#    con_val = cont_val_ori\n",
    "\n",
    "print(train_cb.shape[0],val_cb.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac3b4f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# binary_val = combine_validation[combine_validation['type']=='binary_validation']\n",
    "# small_b_val = small_data(binary_val,subsample=binary_val.shape[0],each = EACH)\n",
    "# cont_val = combine_validation[combine_validation['type']=='continuous_validation']\n",
    "# small_c_val = small_data(cont_val,subsample=binary_val.shape[0],each = EACH)\n",
    "# val_cb = pd.concat([small_b_val,small_c_val],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb770c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# val_cb['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93e798",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "train_cb = preprocess(train_cb,CUTOFF)\n",
    "print('Train Data:')\n",
    "check_bad_letters(train_cb)\n",
    "\n",
    "\n",
    "val_cb = preprocess(val_cb,CUTOFF)\n",
    "print('External Validation Data:')\n",
    "check_bad_letters(val_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae602dc8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# exVal = preprocess(exVal,CUTOFF)\n",
    "# print('External Validation Data:')\n",
    "# check_bad_letters(exVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db16be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# small_train = subsample(train_cb,num_samples=10000//7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c8fd9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# df = train_cb\n",
    "# type_col='type'\n",
    "# project_col='Project'\n",
    "# id_col='id'\n",
    "# num_samples=500\n",
    "# type1_df = df[df[type_col] == 'binary_training']\n",
    "# type2_df = df[df[type_col] == 'continuous_training']\n",
    "\n",
    "# type1_samples = []\n",
    "# type2_samples = []\n",
    "\n",
    "# # For type 1\n",
    "# for project in type1_df[project_col].unique():\n",
    "#     project_df = type1_df[type1_df[project_col] == project]\n",
    "#     num_ids = project_df[id_col].nunique()\n",
    "#     for id_ in project_df[id_col].unique():\n",
    "#         id_df = project_df[project_df[id_col] == id_]\n",
    "#         samples = id_df.sample(min(len(id_df), num_samples // num_ids), replace=False)\n",
    "#         type1_samples.append(samples)\n",
    "\n",
    "# # For type 2\n",
    "# for project in type2_df[project_col].unique():\n",
    "#     project_df = type2_df[type2_df[project_col] == project]\n",
    "#     num_ids = project_df[id_col].nunique()\n",
    "#     num_samples_project = num_samples // num_ids if num_ids <= 5 else 100\n",
    "#     for id_ in project_df[id_col].unique():\n",
    "#         id_df = project_df[project_df[id_col] == id_]\n",
    "#         samples = id_df.sample(min(len(id_df), num_samples_project), replace=False)\n",
    "#         type2_samples.append(samples)\n",
    "\n",
    "# resampled_df = pd.concat(type1_samples + type2_samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91f09e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(small_train['Project'].value_counts())\n",
    "# print(small_train['type'].value_counts())\n",
    "# print(small_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c66a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# binary_tr = train_cb[train_cb['type']=='binary_training']\n",
    "# binary_tr.groupby('Project')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b088e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# cont_tr = train_cb[train_cb['type']=='continuous_training']\n",
    "# cont_tr.groupby('Project')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4e1af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_cb.groupby('Project')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ff455",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#cont_train = cont_train.reindex(columns=val_cb.columns)\n",
    "\n",
    "val_cb_re = val_cb.reindex(columns=val_cb.columns)\n",
    "input_cb = pd.concat([train_cb, val_cb_re], axis=0)\n",
    "\n",
    "aalens_df = input_cb.groupby('antigen_index')['aalens'].mean().astype(int)\n",
    "len_dict = aalens_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddce2f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# round(val_cb.shape[0]*0.1)//7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8504b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# SampleDataset(val_cb, NPY_DIR, CDR3h_hard_dict, Vh_hard_dict, subsample_ratio=EX_SUBSAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba856a37",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# EX_SUBSAMPLE1 = 1\n",
    "# EX_SUBSAMPLE2 = 0\n",
    "# IN_SUBSAMPLE1 = 0.3\n",
    "# IN_SUBSAMPLE2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351c686",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_Loader = InLoader(train_cb, NPY_DIR,CDR3h_dict,Vh_dict,train_ratio=0.8, subsample_ratio1=IN_SUBSAMPLE1,subsample_ratio2 = 0,by_project1=False, by_project2=True,seed=SEED)\n",
    "train_Loader_c = InLoader(train_cb, NPY_DIR,CDR3h_dict,Vh_dict,train_ratio=0.8, subsample_ratio1=0,subsample_ratio2 = IN_SUBSAMPLE2,by_project1=False, by_project2=True,seed=SEED)\n",
    "\n",
    "#trainc_Loader = InLoader(cont_train, NPY_DIR,CDR3h_dict,Vh_dict,train_ratio=0.8, subsample_ratio=1,seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1bf6a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#EX_SUBSAMPLE1 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c2553",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val_dataset = SampleDataset(val_cb, NPY_DIR, CDR3h_hard_dict, Vh_hard_dict, subsample_ratio_1=EX_SUBSAMPLE1,subsample_ratio_2=EX_SUBSAMPLE2)\n",
    "val_loader = DataLoader(val_dataset, 1, sampler=GroupShuffleSampler(val_dataset,val_dataset.group_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c75d2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(Fore.RED+'Checking sample size...')\n",
    "train_dataset = train_Loader.get_train_dataset()\n",
    "trainc_dataset = train_Loader_c.get_train_dataset()\n",
    "\n",
    "print(Fore.BLUE+'training data:')\n",
    "print(train_dataset.dataframe['type'].value_counts())\n",
    "print(trainc_dataset.dataframe['type'].value_counts())\n",
    "\n",
    "print(Fore.GREEN+'validation data:')\n",
    "print(val_dataset.dataframe['type'].value_counts())\n",
    "print(Fore.RESET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f6f6e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_dataset = SampleDataset(train_cb,NPY_DIR, CDR3h_hard_dict, Vh_hard_dict, subsample_ratio=0.01,by_project=False)\n",
    "# print(train_dataset.dataframe.shape)\n",
    "# train_dataset.dataframe['antigen_index'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373123a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parent_dir = '/project/DPDS/Wang_lab/shared/BCR_antigen/data/output'\n",
    "out_dir = os.path.join(parent_dir,get_now()+'_tag'+str(TAG))\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass\n",
    "print(out_dir,'is created!')\n",
    "model_dir = out_dir +'/model_comb'\n",
    "try:\n",
    "    os.mkdir(model_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "print(model_dir,'is created!')\n",
    "res_dir = out_dir +'/process_res'\n",
    "try:\n",
    "    os.mkdir(res_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "print(res_dir,'is created!')\n",
    "\n",
    "with open(out_dir+\"/para.txt\", \"w\") as file:\n",
    "    # Write the text to the file\n",
    "    file.write(f\"batch size: {BATCH_SIZE}\\ninput label:{INPUT}\\nEpoch max:{EPOCH}\\nLearning rate:{LR}\\nCut off:{CUTOFF}\\ntag:{TAG}\\ndelta:{T}\\nLambda:{LAMBDA}\\nweight:{w}\\nlimit:{LIMIT}\\nverbose:{VERBOSE}\\ngroup_size:{GROUP_SIZE}\\nmodel:{MODEL}\\nmax number of antigen:{Max_num}\\number of channels:{CHANNEL_ANTIGEN}\\nsmall_sample:{Small_sample}\\nexternal subsample:{EX_SUBSAMPLE1,EX_SUBSAMPLE2}\\ninternal subsample:{IN_SUBSAMPLE1,IN_SUBSAMPLE2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbfa2f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_mix = mix_model()#.half()\n",
    "if MODEL is not None:\n",
    "    checkpoint = torch.load('/project/DPDS/Wang_lab/shared/BCR_antigen/data/output/'+ MODEL)\n",
    "    model_mix.load_state_dict(checkpoint)\n",
    "model_mix.to(device)\n",
    "optimizer = torch.optim.Adam(model_mix.parameters(),lr=LR)\n",
    "scheduler = ReduceLROnPlateau(optimizer,mode = 'min',factor=0.1,patience =5,verbose=False,threshold_mode='rel',threshold=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc153682",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('batch size:',BATCH_SIZE,'EPOCH:', EPOCH,'Learning rate:',LR)\n",
    "start_time = time.time()\n",
    "print('Initialling ...')\n",
    "# b_train_dataset = binary_loader.get_train_dataset()\n",
    "# b_val_dataset = binary_loader.get_val_dataset()\n",
    "# b_train_loader = DataLoader(b_train_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(b_train_dataset, b_train_dataset.group_ids))\n",
    "# b_val_loader = DataLoader(b_val_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(b_val_dataset, b_val_dataset.group_ids))\n",
    "train_loader,in_val_loader = gen_train_val_loader(train_Loader)\n",
    "init_train_loss,*_ = val_epoch(train_loader,train_cb,model_mix,loss_function,verbose = VERBOSE)\n",
    "init_val_loss,init_val_accu_biased, init_val_acc,*_ = val_epoch(in_val_loader,train_cb,model_mix,loss_function,verbose = VERBOSE)\n",
    "init_ex_loss,init_ex_accu_biased,init_ex_acc,res_init,grouped_ini = val_epoch(val_loader,val_cb,model_mix,loss_function,verbose = VERBOSE)\n",
    "accuracy_by_type_ini = grouped_ini.groupby('type').mean()\n",
    "#init_ex_c_loss,init_ex_c_accu_biased,init_ex_c_acc,res_init_c,grouped_ini_c = val_epoch(c_ex_loader,model_mix,loss_function,verbose = VERBOSE)\n",
    "\n",
    "print('Epoch: -1 train loss: '+str(init_train_loss)+' val loss: '+str(init_val_loss)+' val accuracy: '+str(init_val_acc)+' ex val accuracy: '+str(init_ex_acc))\n",
    "print(' binary accuracy:' +str(accuracy_by_type_ini['binary_validation']))\n",
    "#print(' continuous accuracy: '+str(accuracy_by_type_ini['continuous_validation']))\n",
    "print('Initial training:',time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5b46d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#res_init['type'] = res_init['record_id'].map(val_cb.set_index('record_id')['type'])\n",
    "#grouped_init = res_init.groupby(['antigen', 'type'])['success'].mean()\n",
    "res_init.to_csv(res_dir+'/initial_accu_all.csv')\n",
    "grouped_ini.to_csv(res_dir+'/initial_acc_per.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e34300",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# grouped_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf3bbd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "state_dict=model_mix.state_dict()\n",
    "torch.save(state_dict,model_dir+\"/Initial_Lr\"+str(LR)+\"tag\"+str(TAG)+\"_bnc.pth\")\n",
    "# res['type'] = res['record_id'].map(val_cb.set_index('record_id')['type'])\n",
    "# grouped = res_init.groupby(['antigen', 'type'])['success'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f6557",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_LOSS = [init_train_loss]\n",
    "val_LOSS = [init_val_loss]\n",
    "val_ACC = [init_val_acc]\n",
    "ex_LOSS = [init_ex_loss]\n",
    "ex_ACC_Bias = [init_ex_accu_biased]\n",
    "ex_ACC = [init_ex_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8718ea9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "binary_ACC=[accuracy_by_type_ini['binary_validation']]\n",
    "#cont_ACC=[accuracy_by_type_ini['continuous_validation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582887c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# EPOCH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8b14c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Start Training...')\n",
    "#MARK HERE\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:',str(epoch))\n",
    "    train_Loader.new_epoch()\n",
    "    start_epoch_time = time.time()\n",
    "    print('binary training...')\n",
    "    train_loader,in_val_loader = gen_train_val_loader(train_Loader)\n",
    "#     train_dataset = inputloader.get_train_dataset()\n",
    "#     val_dataset = inputloader.get_val_dataset()\n",
    "#     train_loader = DataLoader(train_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(train_dataset, train_dataset.group_ids))\n",
    "#     val_loader = DataLoader(val_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(val_dataset, val_dataset.group_ids))\n",
    "    train_loss,*_ = train_epoch(train_loader,model_mix,loss_function,optimizer,clip_value=CLIP,verbose = VERBOSE)\n",
    "    in_val_loss,in_val_accu_biased,in_val_accuracy,*_ = val_epoch(in_val_loader,train_cb,model_mix,loss_function,verbose = VERBOSE)\n",
    "#    print('continuous training...')\n",
    "#    trainc_loader,inc_val_loader = gen_train_val_loader(trainc_Loader)\n",
    "#     train_dataset = inputloader.get_train_dataset()\n",
    "#     val_dataset = inputloader.get_val_dataset()\n",
    "#     train_loader = DataLoader(train_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(train_dataset, train_dataset.group_ids))\n",
    "#     val_loader = DataLoader(val_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(val_dataset, val_dataset.group_ids))\n",
    "#    trainc_loss,*_ = train_epoch(trainc_loader,model_mix,loss_function,optimizer,clip_value=CLIP,verbose = VERBOSE)\n",
    "#    inc_val_loss,inc_val_accu_biased,inc_val_accuracy,*_ = val_epoch(inc_val_loader,cont_train,model_mix,loss_function,verbose = VERBOSE)\n",
    "#    print()\n",
    "    print('binary validating...')\n",
    "    if (0 < EX_SUBSAMPLE1 < 1) or (0 < EX_SUBSAMPLE2 < 1):\n",
    "        val_dataset = SampleDataset(val_cb, NPY_DIR, CDR3h_hard_dict, Vh_hard_dict, subsample_ratio_1=EX_SUBSAMPLE1,subsample_ratio_2=EX_SUBSAMPLE2)\n",
    "        val_loader = DataLoader(val_dataset, 1, sampler=GroupShuffleSampler(val_dataset,val_dataset.group_ids))\n",
    "    ex_loss,ex_accu_biased,ex_accuracy,res_ex,group_ex = val_epoch(val_loader,val_cb,model_mix,loss_function,verbose=VERBOSE)\n",
    "    accuracy_by_type = group_ex.groupby('type').mean()\n",
    "    print('Epoch '+str(epoch)+' train loss: '+str(train_loss)+' val accuracy: '+str(in_val_accuracy)+ ' external val loss: '+str(ex_loss)+' ex accu biased: '+str(ex_accu_biased)+' external val accuracy: '+str(ex_accuracy))\n",
    "    print(' binary accuracy:' +str(accuracy_by_type['binary_validation']))\n",
    "#    print(' continuous accuracy: '+str(accuracy_by_type['continuous_validation']))\n",
    "\n",
    "    res_ex.to_csv(res_dir+'/ex_accuracy_tag'+str(TAG)+'_Epoch'+str(epoch)+'_Lr'+str(LR)+'_b_all.csv')\n",
    "    group_ex.to_csv(res_dir+'/ex_accuracy_tag'+str(TAG)+'_Epoch'+str(epoch)+'_Lr'+str(LR)+'_b_per_antigen.csv')\n",
    "\n",
    "#    scheduler.step(val_loss) ##should I try scheduler.step(-val_accuracy)?\n",
    "\n",
    "    train_LOSS.append(train_loss)\n",
    "    val_LOSS.append(in_val_loss)\n",
    "    val_ACC.append(in_val_accuracy)\n",
    "\n",
    "    ex_LOSS.append(ex_loss)\n",
    "    ex_ACC.append(ex_accuracy)\n",
    "    ex_ACC_Bias.append(ex_accu_biased)\n",
    "    binary_ACC.append(accuracy_by_type['binary_validation'])\n",
    "#    cont_ACC.append(accuracy_by_type['continuous_validation'])\n",
    "#     ex_cont_ACC_Biased.append(c_ex_accu_biased)\n",
    "#     ex_binary_ACC.append(b_ex_accuracy)\n",
    "#     ex_cont_ACC.append(c_ex_accuracy)\n",
    "    state_dict=model_mix.state_dict()\n",
    "    torch.save(state_dict,model_dir+\"/Batch\"+str(BATCH_SIZE)+\"Lr\"+str(LR)+\"Epoch\"+str(epoch)+\"tag\"+str(TAG)+\"_b.pth\")\n",
    "\n",
    "    train_Loader_c.new_epoch()\n",
    "    print('cont training...')\n",
    "    trainc_loader,inc_val_loader = gen_train_val_loader(train_Loader_c)\n",
    "#     train_dataset = inputloader.get_train_dataset()\n",
    "#     val_dataset = inputloader.get_val_dataset()\n",
    "#     train_loader = DataLoader(train_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(train_dataset, train_dataset.group_ids))\n",
    "#     val_loader = DataLoader(val_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(val_dataset, val_dataset.group_ids))\n",
    "    trainc_loss,*_ = train_epoch(trainc_loader,model_mix,loss_function,optimizer,clip_value=CLIP,verbose = VERBOSE)\n",
    "    inc_val_loss,inc_val_accu_biased,inc_val_accuracy,*_ = val_epoch(inc_val_loader,train_cb,model_mix,loss_function,verbose = VERBOSE)\n",
    "#    print('continuous training...')\n",
    "#    trainc_loader,inc_val_loader = gen_train_val_loader(trainc_Loader)\n",
    "#     train_dataset = inputloader.get_train_dataset()\n",
    "#     val_dataset = inputloader.get_val_dataset()\n",
    "#     train_loader = DataLoader(train_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(train_dataset, train_dataset.group_ids))\n",
    "#     val_loader = DataLoader(val_dataset, BATCH_SIZE, sampler=GroupShuffleSampler(val_dataset, val_dataset.group_ids))\n",
    "#    trainc_loss,*_ = train_epoch(trainc_loader,model_mix,loss_function,optimizer,clip_value=CLIP,verbose = VERBOSE)\n",
    "#    inc_val_loss,inc_val_accu_biased,inc_val_accuracy,*_ = val_epoch(inc_val_loader,cont_train,model_mix,loss_function,verbose = VERBOSE)\n",
    "#    print()\n",
    "    print('binary plus validating...')\n",
    "    if (0 < EX_SUBSAMPLE1 < 1) or (0 < EX_SUBSAMPLE2 < 1):\n",
    "        val_dataset = SampleDataset(val_cb, NPY_DIR, CDR3h_hard_dict, Vh_hard_dict, subsample_ratio_1=EX_SUBSAMPLE1,subsample_ratio_2=EX_SUBSAMPLE2)\n",
    "        val_loader = DataLoader(val_dataset, 1, sampler=GroupShuffleSampler(val_dataset,val_dataset.group_ids))\n",
    "    ex_loss,ex_accu_biased,ex_accuracy,res_ex,group_ex = val_epoch(val_loader,val_cb,model_mix,loss_function,verbose=VERBOSE)\n",
    "    accuracy_by_type = group_ex.groupby('type').mean()\n",
    "    print('Epoch '+str(epoch)+' train loss: '+str(train_loss)+' val accuracy: '+str(in_val_accuracy)+ ' external val loss: '+str(ex_loss)+' ex accu biased: '+str(ex_accu_biased)+' external val accuracy: '+str(ex_accuracy))\n",
    "    print(' binary accuracy:' +str(accuracy_by_type['binary_validation']))\n",
    "#    print(' continuous accuracy: '+str(accuracy_by_type['continuous_validation']))\n",
    "\n",
    "    res_ex.to_csv(res_dir+'/ex_accuracy_tag'+str(TAG)+'_Epoch'+str(epoch)+'_Lr'+str(LR)+'_bplus_all.csv')\n",
    "    group_ex.to_csv(res_dir+'/ex_accuracy_tag'+str(TAG)+'_Epoch'+str(epoch)+'_Lr'+str(LR)+'_bplus_per_antigen.csv')\n",
    "\n",
    "#    scheduler.step(val_loss) ##should I try scheduler.step(-val_accuracy)?\n",
    "\n",
    "    train_LOSS.append(trainc_loss)\n",
    "    val_LOSS.append(inc_val_loss)\n",
    "    val_ACC.append(inc_val_accuracy)\n",
    "\n",
    "    ex_LOSS.append(ex_loss)\n",
    "    ex_ACC.append(ex_accuracy)\n",
    "    ex_ACC_Bias.append(ex_accu_biased)\n",
    "    binary_ACC.append(accuracy_by_type['binary_validation'])\n",
    "#    cont_ACC.append(accuracy_by_type['continuous_validation'])\n",
    "#     ex_cont_ACC_Biased.append(c_ex_accu_biased)\n",
    "#     ex_binary_ACC.append(b_ex_accuracy)\n",
    "#     ex_cont_ACC.append(c_ex_accuracy)\n",
    "    state_dict=model_mix.state_dict()\n",
    "    torch.save(state_dict,model_dir+\"/Batch\"+str(BATCH_SIZE)+\"Lr\"+str(LR)+\"Epoch\"+str(epoch)+\"tag\"+str(TAG)+\"_bplus.pth\")\n",
    "\n",
    "    if math.isnan(train_loss):\n",
    "        state_dict = model_mix.state_dict()\n",
    "        torch.save(state_dict,model_dir+\"/Error_model_epoch\"+str(epoch)+\"/Batch\"+str(BATCH_SIZE)+\"_Lr\"+str(LR)+\"_tag\"+str(TAG)+\".pth\")\n",
    "        break\n",
    "        print('Epoch:',epoch,'train_loss:',train_LOSS[-1])\n",
    "#     if val_accuracy > 0.98:\n",
    "#         print('Validation Accuracy reached 0.98!')\n",
    "#         break\n",
    "    print('All_through time: '+str(time.time()-start_epoch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ad47a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#MARK HERE\n",
    "\n",
    "# STOP HERE and DO NOT OVERWRITE lossTable!! ATTACH THE NEW RESUlTS to it!\n",
    "epoch_ls = [i for i in range(len(train_LOSS)-1)]\n",
    "epoch_ls.insert(0,-1)\n",
    "lossTable =pd.DataFrame({'Epoch':epoch_ls,'train_loss':train_LOSS,\n",
    "                        'in_val_loss':val_LOSS, #trainc_LOSS,#'cont10x_val_loss':val_cont10_LOSS,'contLib_val_loss':val_contLib_LOSS,\n",
    "                         # 'binary_val_biased_accuracy':val_ACC,# 'cont10x_val_biased_accuracy':val_cont10_ACC_Biased,'contLib_val_biased_accuracy':val_contLib_ACC_Biased,\n",
    "                         'in_val_accuracy':val_ACC,\n",
    "#                         'in_c_val_accuracy':valc_ACC,\n",
    "#                          'binary_ex_loss_before':ex_binary_LOSS_BEFORE,'cont_ex_loss_before':ex_cont_LOSS_BEFORE,\n",
    "#                          'binary_ex_biased_accuracy_before':ex_binary_ACC_Biased_BEFORE,'cont_ex_biased_accuracy_before':ex_cont_ACC_Biased_BEFORE,\n",
    "#                          'binary_ex_accuracy_before':ex_binary_ACC_BEFORE,'cont_ex_accuracy_before':ex_cont_ACC_BEFORE,# 'cont10x_val_accuracy':val_cont10_ACC,'contLib_val_accuracy':val_contLib_ACC,\n",
    "                        'ex_loss':ex_LOSS,#'cont_ex_loss':ex_cont_LOSS,\n",
    "                         'ex_biased_accuracy':ex_ACC_Bias,#'cont_ex_biased_accuracy':ex_cont_ACC_Biased,\n",
    "                         'ex_accuracy':ex_ACC,\n",
    "                         'binary_accuracy':binary_ACC,\n",
    "#                         'continuous_accuracy':cont_ACC#,'cont_ex_accuracy':ex_cont_ACC\n",
    "                        })#Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029f335b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# lossTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a52744",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "lossTable.to_csv(out_dir+'/lossTable_tag'+str(TAG)+'_Epoch'+str(EPOCH)+'_Lr'+str(LR)+'.csv')\n",
    "print(lossTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f667b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "# axes[0,0].plot(lossTable['binary_ex_loss_before'],color='red',label='binary_ex_loss_before')\n",
    "# axes[0,0].plot(lossTable['binary_ex_loss'],color='blue',label='binary_ex_loss')\n",
    "# axes[0,0].legend()\n",
    "# axes[0,0].set_xlabel('epoch')\n",
    "# axes[0,0].set_ylabel('loss')\n",
    "# axes[0,1].plot(lossTable['cont_ex_loss_before'],color='red',label='cont_ex_loss_before')\n",
    "# axes[0,1].plot(lossTable['cont_ex_loss'],color='blue',label='cont_ex_loss')\n",
    "# axes[0,1].legend()\n",
    "# axes[0,1].set_xlabel('epoch')\n",
    "# axes[0,1].set_ylabel('loss')\n",
    "# axes[1,0].plot(lossTable['binary_ex_accuracy_before'],color='red',label='binary_ex_accuracy_before')\n",
    "# axes[1,0].plot(lossTable['binary_ex_accuracy'],color='blue',label='binary_ex_accuracy')\n",
    "# axes[1,0].legend()\n",
    "# axes[1,0].set_xlabel('epoch')\n",
    "# axes[1,0].set_ylabel('accuracy')\n",
    "# axes[1,1].plot(lossTable['cont_ex_accuracy_before'],color='red',label='cont_ex_accuracy_before')\n",
    "# axes[1,1].plot(lossTable['cont_ex_accuracy'],color='blue',label='cont_ex_accuracy')\n",
    "# axes[1,1].legend()\n",
    "# axes[1,1].set_xlabel('epoch')\n",
    "# axes[1,1].set_ylabel('accuracy')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# plt.savefig(out_dir+'/lossPlot_tag'+str(TAG)+'_Epoch'+str(EPOCH)+'_Lr'+str(LR)+'.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(axis='both', which='both', left=True, labelleft=True, bottom=True, labelbottom=True)\n",
    "lossTable.plot(x='Epoch',y='train_loss',kind='line',ax=axs[0,0])\n",
    "lossTable.plot(x='Epoch',y='in_val_loss',kind='line',ax=axs[0,1])\n",
    "lossTable.plot(x='Epoch',y='in_val_accuracy',kind='line',ax=axs[0,2])\n",
    "#lossTable.plot(x='Epoch',y='in_c_val_accuracy',kind='line',ax=axs[1,0])\n",
    "lossTable.plot(x='Epoch',y='ex_loss',kind='line',ax=axs[1,0])\n",
    "#lossTable.plot(x='Epoch',y='ex_biased_accuracy',kind='line',ax=axs[1,1])\n",
    "lossTable.plot(x='Epoch',y='ex_accuracy',kind='line',ax=axs[1,1])\n",
    "lossTable.plot(x='Epoch',y='binary_accuracy',kind='line',ax=axs[1,2])\n",
    "#lossTable.plot(x='Epoch',y='continuous_accuracy',kind='line',ax=axs[2,1])\n",
    "#lossTable.plot(x='Epoch',y='cont_ex_accuracy',kind='line',ax=axs[2,2])\n",
    "plt.savefig(out_dir+'/lossPlot_tag'+str(TAG)+'_Epoch'+str(EPOCH)+'_Lr'+str(LR)+'.png',dpi=200)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pytorch_new",
   "language": "python",
   "name": "pytorch_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
